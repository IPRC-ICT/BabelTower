__global__ void add_100 ( int numElements , int * data ) { if ( blockIdx . x < numElements ) { data [ blockIdx . x ] += 100 ; } }
__global__ void get_ev ( double * old_arr , double * new_arr ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; new_arr [ tid ] = old_arr [ tid ] ; }
__global__ void square ( int * array , int arrayCount ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; if ( idx < arrayCount ) { array [ idx ] *= array [ idx ] ; } }
__global__ void add ( int n , float * x , float * y ) { int i = threadIdx . x ; if ( i < n ) y [ i ] = x [ i ] + y [ i ] ; }
__global__ void scale_dev ( float * array , float scale , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) { array [ idx ] *= scale ; } return ; }
__global__ void allAddInplaceKernel ( double * arr , double alpha , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { arr [ i ] += alpha ; } }
__global__ void memsetCudaInt ( int * data , int val , int N ) { unsigned int index = blockDim . x * blockIdx . x + threadIdx . x ; if ( index >= N ) { return ; } data [ index ] = val ; }
__global__ void initialArray0 ( int tasks , int * f3 ) { for ( int i = blockIdx . x * blockDim . x + threadIdx . x ; i < tasks ; i += blockDim . x * gridDim . x ) { f3 [ i ] = 0 ; } }
__global__ void VectorAdd ( float * arrayA , float * arrayB , float * output ) { int idx = threadIdx . x ; output [ idx ] = arrayA [ idx ] + arrayB [ idx ] ; }
__global__ void test ( float * input , const int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( tid == 0 ) { input [ tid ] = 0 ; } }
__global__ void set_sorting_offset ( const int nrows , const int ncols , int * offsets ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; if ( tid <= ncols ) offsets [ tid ] = tid * nrows ; return ; }
__global__ void dotKernel ( float * c , float * a , float * b ) { int t_id = blockIdx . x * blockDim . x + threadIdx . x ; c [ t_id ] = a [ t_id ] * b [ t_id ] ; }
__global__ void matDiagAddInplaceKernel ( double * mat , double alpha , int dim ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < dim ) { mat [ i * dim + i ] += alpha ; } }
__global__ void cudaAddCorrAndCorrection ( float * L , float * r , int N ) { int u = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( u >= N ) return ; L [ u ] -= r [ u ] ; }
__global__ void fill_kernel ( int N , float ALPHA , float * X , int INCX ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) X [ i * INCX ] = ALPHA ; }
__global__ void scal_kernel ( int N , float ALPHA , float * X , int INCX ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) X [ i * INCX ] *= ALPHA ; }
__global__ void PSIfill ( float * array , int conv_length , int maxThreads ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= maxThreads ) return ; array [ i ] = array [ i % conv_length ] ; }
__global__ void gpu_add ( float * c , float * a , float * b , int n ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; c [ j ] = a [ j ] + b [ j ] ; }
__global__ void mul_Scalar_matrix ( float * a , float value , float * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] * value ; }
__global__ void initWith ( float num , float * a , int N ) { int index = threadIdx . x + blockIdx . x * blockDim . x ; int stride = blockDim . x * gridDim . x ; for ( int i = index ; i < N ; i += stride ) { a [ i ] = num ; } }
__global__ void zeroIndices ( long * vec_out , const long N ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < N ) { vec_out [ idx ] = vec_out [ idx ] - vec_out [ 0 ] ; } }
__global__ void saxpy_gpu ( const int dim , float a , float * x , float * y ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < dim ) y [ i ] = a * x [ i ] + y [ i ] ; }
__global__ void getCanBusData ( int * canData , int size , int nthreads , int nblocks ) { int i ; int idx = blockIdx . x * blockDim . x + threadIdx . x ; for ( i = idx ; i < size ; i += nthreads * nblocks ) { canData [ idx ] += 1 ; } }
__global__ void sum_array_1Dgrid_1Dblock ( float * a , float * b , float * c , int nx ) { int gid = blockDim . x * blockIdx . x + threadIdx . x ; c [ gid ] = a [ gid ] + b [ gid ] ; }
__global__ void matColMeanDiv ( double * buf , int m , int n , double * tmp ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { buf [ i ] = tmp [ i ] / m ; } }
__global__ void dmul_Scalar_matrix ( double * a , double value , double * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] * value ; }
__global__ void countRangesGlobal ( int size , int * A , int * B ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= size ) return ; int x = A [ i ] / 100 ; B [ x ] += 1 ; }
__global__ void dsubtract_matrix ( double * a , double * b , double * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] - b [ idx ] ; }
__global__ void add_arrays ( int n , float * x , float * y , float * z ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < n ) { z [ i ] = x [ i ] + y [ i ] ; } }
__global__ void sum_arrays_gpu ( int * a , int * b , int * c , int size ) { int index = blockDim . x * blockIdx . x + threadIdx . x ; if ( index < size ) c [ index ] = a [ index ] + b [ index ] ; }
__global__ void iKernel ( float * A , float * B , float * C , const int N ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < N ) C [ i ] = A [ i ] + B [ i ] ; }
__global__ void intMultiply ( int * result , const int * val1 , const int val2 , const unsigned int size ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; if ( i < size ) { result [ blockIdx . x ] = val1 [ blockIdx . x ] * val2 ; } }
__global__ void doubleArrayScalarDivideKernel ( double * d_in , int * d_out , int length , double scalar ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = ( int ) ( d_in [ tid ] / scalar ) ; } }
__global__ void addKernel ( int * c , const int * a , const int * b ) { int x = threadIdx . x ; int y = threadIdx . y ; int i = y * ( blockDim . x ) + x ; c [ i ] = a [ i ] + b [ i ] ; }
__global__ void activate_array_leaky_kernel ( float * x , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < n ) { float val = x [ index ] ; x [ index ] = ( val > 0 ) ? val : val / 10 ; } }
__global__ void logistic ( unsigned int n , float a , float * x , float * z ) { unsigned int myId = blockDim . x * blockIdx . x + threadIdx . x ; if ( myId < n ) z [ myId ] = a * x [ myId ] * ( 1 - x [ myId ] ) ; }
__global__ void add_kernel ( float * inputleft , float * inputright , float * output , int count ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx >= count ) return ; output [ idx ] = inputleft [ idx ] + inputright [ idx ] ; }
__global__ void mul_kernel ( int N , float * X , int INCX , float * Y , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) Y [ i * INCY ] *= X [ i * INCX ] ; }
__global__ void pathPlan ( int * devSpeed , int * devSteer , int size ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; while ( tid < size ) { devSpeed [ tid ] += 1 ; devSteer [ tid ] += 1 ; tid += blockDim . x * gridDim . x ; } }
__global__ void mult_add_into_kernel ( int n , float * a , float * b , float * c ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { c [ i ] += a [ i ] * b [ i ] ; } }
__global__ void InitReduction ( bool * flags , int voxelCount , int * reduction , int reductionSize ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; if ( tid >= reductionSize ) { return ; } reduction [ tid ] = ( tid < voxelCount ) ? flags [ tid ] : 0 ; }
__global__ void Kernel_Function_update_sgd ( float lr , float * dev_parameter , float * dev_gradient , int size ) { int tid = blockDim . x * blockIdx . x + threadIdx . x ; int N = size ; while ( tid < N ) { dev_parameter [ tid ] -= lr * dev_gradient [ tid ] ; tid += gridDim . x * blockDim . x ; } }
__global__ void operacionKernelGPU ( float * u , float * lu , float u_m , float u_d , int n ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < n ) lu [ idx ] = ( u [ idx ] - u_m ) / u_d ; }
__global__ void gpu_add ( float * c , float * a , float * b , int n ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; int m = gridDim . x * blockDim . x ; for ( int k = j ; k < n ; k += m ) { c [ k ] = a [ k ] + b [ k ] ; } }
__global__ void squareKernel ( float * d_in , float * d_out , int N ) { const unsigned int lid = threadIdx . x ; const unsigned int gid = blockIdx . x * blockDim . x + lid ; if ( gid < N ) { d_out [ gid ] = pow ( d_in [ gid ] / ( d_in [ gid ] - 2.3 ) , 3 ) ; } }
__global__ void doubleArrayVectorAddKernel ( double * d_in_a , double * d_in_b , double * d_out , int length ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = d_in_a [ tid ] + d_in_b [ tid ] ; } }
__global__ void fill_matrix ( double * const A , const int rows , const int cols ) { const int row = blockIdx . y * blockDim . y + threadIdx . y , col = blockIdx . x * blockDim . x + threadIdx . x ; if ( row < rows && col < cols ) { A [ row * cols + col ] = row ; } }
__global__ void evenoddincrement ( float * g_data , int even_inc , int odd_inc ) { int tx = threadIdx . x + blockIdx . x * blockDim . x ; if ( ( tx % 2 ) == 0 ) { g_data [ tx ] += even_inc ; } else { g_data [ tx ] += odd_inc ; } }
__global__ void copy_kernel ( int N , float * X , int OFFX , int INCX , float * Y , int OFFY , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) Y [ i * INCY + OFFY ] = X [ i * INCX + OFFX ] ; }
__global__ void clearLabel ( float * prA , float * prB , unsigned int num_nodes , float base ) { unsigned int id = blockDim . x * blockIdx . x + threadIdx . x ; if ( id < num_nodes ) { prA [ id ] = base + prA [ id ] * 0.85 ; prB [ id ] = 0 ; } }
__global__ void delay_kernel ( int * N_mobil , int * Tau , int dia ) { int N = N_mobil [ 0 ] ; int id = blockIdx . x * blockDim . x + threadIdx . x ; if ( id < N ) { if ( Tau [ id ] > 0 ) Tau [ id ] = Tau [ id ] - 1 ; } }
__global__ void resetHeapKernel ( int * heap , int * heapPtr , int numBlock ) { int index = threadIdx . x + blockDim . x * blockIdx . x ; if ( index >= numBlock ) return ; if ( index == 0 ) heapPtr [ 0 ] = numBlock - 1 ; heap [ index ] = numBlock - index - 1 ; }
__global__ void pow_kernel ( int N , float ALPHA , float * X , int INCX , float * Y , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) Y [ i * INCY ] = pow ( X [ i * INCX ] , ALPHA ) ; }
__global__ void kComputeActs ( const float * d_nets , float * d_acts ) { int un_idx = blockIdx . x * blockDim . x + threadIdx . x ; float tact = 1.0f / ( 1.0f + expf ( - d_acts [ un_idx ] ) ) ; __syncthreads ( ) ; d_acts [ un_idx ] = tact ; }
__global__ void transposeNaive ( int * vector , int * transposed , int size ) { int column = threadIdx . x + blockDim . x * blockIdx . x ; int row = threadIdx . y + blockDim . x * blockIdx . y ; if ( row < size && column < size ) transposed [ row + column * size ] = vector [ column + row * size ] ; }
__global__ void compute_array_square ( float * array , float * outArray , int size ) { int thread_index = threadIdx . x + blockIdx . x * blockDim . x ; int num_threads = blockDim . x * gridDim . x ; for ( int i = 0 ; i < size ; i += num_threads ) { int index = i + thread_index ; if ( index < size ) { outArray [ index ] = array [ index ] * array [ index ] ; } } }
__global__ void testInt1 ( const int * input , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } int sum ; for ( int i = 0 ; i < 3000 * 4 ; i ++ ) { if ( input [ i ] == 0 ) { sum ++ ; } } }
__global__ void incKernel ( int * g_out , int * g_in , int N , int inner_reps ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) { for ( int i = 0 ; i < inner_reps ; ++ i ) { g_out [ idx ] = g_in [ idx ] + 1 ; } } }
__global__ void forward_dropout_layer ( float * input , int size , float * rand , float prob , float scale ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id < size ) input [ id ] = ( rand [ id ] < prob ) ? 0 : input [ id ] * scale ; }
__global__ void boundaryCorrectIndexesKernel ( int * d_in , int * d_out , int length , int N ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { if ( d_in [ tid ] > N ) { d_out [ tid ] = N ; } else { d_out [ tid ] = d_in [ tid ] ; } } }
__global__ void upsweep_scan ( int twod , int N , int * output ) { int twod1 = twod * 2 ; int idx = ( blockIdx . x * blockDim . x + threadIdx . x ) * twod1 ; if ( idx + twod1 - 1 < N ) output [ idx + twod1 - 1 ] += output [ idx + twod - 1 ] ; }
__global__ void Blending_Kernel ( unsigned char * aR1 , unsigned char * aR2 , unsigned char * aRS , int size ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < size ) aRS [ index ] = 0.5 * aR1 [ index ] + 0.5 * aR2 [ index ] ; }
__global__ void matVecRowSubInplaceKernel ( double * mat , const double * vec , int m , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < m * n ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] -= vec [ j ] ; } }
__global__ void matVecColAddInplaceKernel ( double * mat , const double * vec , int m , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < m * n ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] += vec [ i ] ; } }
__global__ void MMDOuterProdComputeWithSum ( float * x_average , int size_x , float * x_outer_prod ) { int block_id = blockIdx . x ; int thread_id = threadIdx . x ; for ( int i = block_id * blockDim . x + thread_id ; i < size_x ; i += gridDim . x * blockDim . x ) { x_outer_prod [ i ] = x_average [ i ] * x_average [ i ] ; } }
__global__ void saxpy_gpu ( float * vecY , float * vecX , float alpha , int n ) { int x , y , i ; x = blockIdx . x * blockDim . x + threadIdx . x ; y = blockIdx . y * blockDim . y + threadIdx . y ; i = y * 1024 + x ; if ( i < n ) vecY [ i ] = alpha * vecX [ i ] + vecY [ i ] ; }
__global__ void set_valid_mask ( const float * score , float score_thr , int * valid_mask , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( score [ tid ] > score_thr ) { valid_mask [ tid ] = 1 ; } else { valid_mask [ tid ] = 0 ; } }
__global__ void copy_swap ( float * f_in , float * f_target , const int L_x ) { const int k_x = threadIdx . x + blockIdx . x * blockDim . x ; if ( k_x >= L_x ) { return ; } float tempval = 0.f ; tempval = f_in [ k_x ] ; f_in [ k_x ] = f_target [ k_x ] ; f_target [ k_x ] = tempval ; }
__global__ void Kernel_Sum_backward_opt2 ( float * db , float * sum , int r_sum , int c ) { unsigned int j = blockDim . x * blockIdx . x + threadIdx . x ; if ( j >= c ) return ; float temp = 0 ; for ( int i = 0 ; i < r_sum ; i ++ ) { temp += sum [ i * c + j ] ; } db [ j ] = temp ; }
__global__ void is_repeat ( int N , int * device_input , int * device_output ) { int idx = blockDim . x * blockIdx . x + threadIdx . x ; if ( idx < N ) { device_output [ idx ] = 0 ; if ( idx + 1 < N && device_input [ idx ] == device_input [ idx + 1 ] ) device_output [ idx ] = 1 ; } }
__global__ void kmeans_average ( int * means , int * counts ) { if ( counts [ blockIdx . x ] == 0 ) means [ blockIdx . x * blockDim . x + threadIdx . x ] = 0 ; else means [ blockIdx . x * blockDim . x + threadIdx . x ] /= counts [ blockIdx . x ] ; }
__global__ void matPerRowDivInplaceKernel ( double * mat , const double * alphas , int m , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < m * n ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] /= ( alphas [ i ] + 10 * 3 ) ; } }
__global__ void compute_new_means ( float * mx , float * my , const float * sx , const float * sy , const int * c ) { const int cluster = threadIdx . x ; const int count = max ( 1 , c [ cluster ] ) ; mx [ cluster ] = sx [ cluster ] / count ; my [ cluster ] = sy [ cluster ] / count ; }
__global__ void copy_array_d2d ( double * * src , double * * dst , int m , int n ) { int i , j ; i = blockIdx . x * blockDim . x + threadIdx . x ; j = blockIdx . y * blockDim . y + threadIdx . y ; if ( i >= 1 && i < m + 1 && j >= 1 && j < n + 1 ) dst [ i ] [ j ] = src [ i ] [ j ] ; }
__global__ void InitCCL ( int labelList [ ] , int reference [ ] , int width , int height ) { int x = blockIdx . x * blockDim . x + threadIdx . x ; int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( x >= width || y >= height ) return ; int id = x + y * width ; labelList [ id ] = reference [ id ] = id ; }
__global__ void cuda_set_sg ( int * sxz , int sxbeg , int szbeg , int jsx , int jsz , int ns , int npml , int nnz ) { int id = threadIdx . x + blockDim . x * blockIdx . x ; if ( id < ns ) sxz [ id ] = nnz * ( sxbeg + id * jsx + npml ) + ( szbeg + id * jsz + npml ) ; }
__global__ void addMatrixGPU ( float * a , float * b , float * c , int N ) { int idx ; int j = threadIdx . x + blockIdx . x * blockDim . x ; int i = threadIdx . y + blockIdx . y * blockDim . y ; if ( ( i < N ) && ( j < N ) ) { idx = i * N + j ; a [ idx ] = b [ idx ] + c [ idx ] ; } }
__global__ void resizedClsScore ( const float * score , const float * score_factors , float * output , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( score [ tid ] == ( -1 ) ) { output [ tid ] = -1 ; } else { output [ tid ] = score [ tid ] * score_factors [ tid ] ; } }
__global__ void l1_kernel ( int n , float * pred , float * truth , float * delta , float * error ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { float diff = truth [ i ] - pred [ i ] ; error [ i ] = abs ( diff ) ; delta [ i ] = ( diff > 0 ) ? 1 : -1 ; } }
__global__ void AddMatrixOnGPU ( float * A , float * B , float * C , int nx , int ny ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; int j = threadIdx . y + blockIdx . y * blockDim . y ; int idx = i * nx + j ; if ( i <= nx && j <= ny ) { C [ idx ] = A [ idx ] + B [ idx ] ; } }
__global__ void LreluForward ( float * srcData , float * dstData , int data_size , float alpha ) { int thread_index = threadIdx . x + blockIdx . x * blockDim . x ; int num_threads = blockDim . x * gridDim . x ; for ( int i = 0 ; i < data_size ; i += num_threads ) { int index = i + thread_index ; if ( index < data_size ) { dstData [ index ] = srcData [ index ] > 0 ? srcData [ index ] : srcData [ index ] * alpha ; } } }
__global__ void filterFFT ( float * FFT , float * filter , int nxprj2 , int nviews , float scale ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; int i = blockIdx . y * blockDim . y + threadIdx . y ; if ( i < nviews && j < nxprj2 ) FFT [ i * nxprj2 + j ] *= filter [ i * nxprj2 + j ] * scale ; }
__global__ void convertFloatToRGBA_kernel ( char * out_image , const float * in_image , int width , int height ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; char temp ; if ( x < width && y < height ) { int IND = y * width + x ; float val = in_image [ IND ] ; temp = 255 ; out_image [ IND ] = temp ; } }
__global__ void convertEdgeMaskToFloatDevice ( float * d_output , unsigned char * d_input , unsigned int width , unsigned int height ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( x >= width || y >= height ) return ; d_output [ y * width + x ] = min ( d_input [ y * width + x ] , d_input [ width * height + y * width + x ] ) ; }
__global__ void gpu_matrix_transpose ( int * mat_in , int * mat_out , unsigned int rows , unsigned int cols ) { unsigned int idx = blockIdx . x * blockDim . x + threadIdx . x ; unsigned int idy = blockIdx . y * blockDim . y + threadIdx . y ; if ( idx < cols && idy < rows ) { unsigned int pos = idy * cols + idx ; unsigned int trans_pos = idx * rows + idy ; mat_out [ trans_pos ] = mat_in [ pos ] ; } }
__global__ void LreluBackward ( float * srcDiff , float * dstDiff , float * srcData , int data_size , float alpha ) { int thread_index = threadIdx . x + blockIdx . x * blockDim . x ; int num_threads = blockDim . x * gridDim . x ; for ( int i = 0 ; i < data_size ; i += num_threads ) { int index = i + thread_index ; if ( index < data_size ) { dstDiff [ index ] = srcDiff [ index ] * ( ( srcData [ index ] > 0 ) + ( srcData [ index ] <= 0 ) * alpha ) ; } } }
__global__ void gpuReduceRecursive ( int * I , int * O , unsigned int n ) { unsigned int tid = threadIdx . x ; unsigned int idx = threadIdx . x + blockIdx . x * blockDim . x ; if ( idx >= n ) return ; int * N = I + blockIdx . x * blockDim . x ; for ( int stride = 1 ; stride < blockDim . x ; stride *= 2 ) { if ( ( tid % ( 2 * stride ) ) == 0 ) N [ tid ] += N [ tid + stride ] ; __syncthreads ( ) ; } if ( tid == 0 ) O [ blockIdx . x ] = N [ 0 ] ; }
__global__ void devidecountInner ( long Xsize , long Ysize , long Zsize , double * p , double * pn , int * pcountinner ) { long tid = threadIdx . x + blockDim . x * blockIdx . x ; while ( tid < Xsize * Ysize * Zsize ) { if ( pcountinner [ tid ] > 1 ) { p [ tid ] = pn [ tid ] / pcountinner [ tid ] ; pn [ tid ] = 0 ; } tid += blockDim . x * gridDim . x ; } }
__global__ void cudaConvertToBits ( int * bit_decisions , unsigned short * bit_stream , int dec_size ) { int dec_index = ( blockIdx . x * blockDim . x ) + threadIdx . x ; int bit_index = dec_index * 2 ; if ( dec_index >= dec_size ) return ; int curr_decision = bit_decisions [ dec_index ] ; bit_stream [ bit_index ] = ( ( curr_decision & 2 ) >> 1 ) ; bit_stream [ bit_index + 1 ] = ( curr_decision & 1 ) ; }
__global__ void copyAliasRow ( int * devMat , int memWidth , int memHeight ) { int devMatX = blockIdx . x * blockDim . x + threadIdx . x + 1 ; devMat [ memWidth * 0 + devMatX ] = devMat [ memWidth * ( memHeight - 2 ) + devMatX ] ; devMat [ memWidth * ( memHeight - 1 ) + devMatX ] = devMat [ memWidth * 1 + devMatX ] ; }
__global__ void circularity ( const int compCount , const int * areaRes , const float * perimeterRes , float * circ ) { int tid = blockDim . x * blockIdx . x + threadIdx . x ; if ( tid < compCount ) { circ [ tid ] = ( 4.0 * 3.14159265359 * ( float ) areaRes [ tid ] ) / ( perimeterRes [ tid ] * perimeterRes [ tid ] ) ; } }
__global__ void devidecount ( long Xsize , long Ysize , long Zsize , double * pint , int * pcount ) { int n = Xsize * Ysize * 2 + ( Ysize - 2 ) * Zsize * 2 + ( Xsize - 2 ) * ( Zsize - 2 ) * 2 ; long tid = threadIdx . x + blockDim . x * blockIdx . x ; while ( tid < n * n ) { if ( pcount [ tid ] > 1 ) { pint [ tid ] /= pcount [ tid ] ; } tid += blockDim . x * gridDim . x ; } }
__global__ void oddevenSort ( int * d_in , int size , int oe_flag , int & d_ch_flag ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; int p = 2 * idx + oe_flag ; if ( p + 1 < size ) { if ( d_in [ p ] > d_in [ p + 1 ] ) { int temp = d_in [ p ] ; d_in [ p ] = d_in [ p + 1 ] ; d_in [ p + 1 ] = temp ; d_ch_flag = 1 ; } } }
__global__ void matmul ( float * a , float * b , float * c , int width ) { float result = 0 ; int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; for ( int k = 0 ; k < width ; k ++ ) { result += a [ row * width + k ] * b [ k * width + col ] ; } c [ row * width + col ] = result ; }
__global__ void cudaKernel_estimateSnr ( const float * corrSum , const int * corrValidCount , const float * maxval , float * snrValue , const int size ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx >= size ) return ; float mean = ( corrSum [ idx ] - maxval [ idx ] * maxval [ idx ] ) / ( corrValidCount [ idx ] - 1 ) ; snrValue [ idx ] = maxval [ idx ] * maxval [ idx ] / mean ; }
__global__ void naive_sgemm_kernel ( float * C , float * A , float * B , long size ) { const long i = blockIdx . x * blockDim . x + threadIdx . x ; const long j = blockIdx . y * blockDim . y + threadIdx . y ; float val = 0.0 ; if ( i >= size || j >= size ) return ; for ( long k = 0 ; k < size ; k ++ ) { val += A [ i * size + k ] * B [ k * size + j ] ; } C [ i * size + j ] += val ; }
__global__ void kernelXor ( unsigned int key , char * input_str_cuda , unsigned char * possible_plaintext_str_cuda , int input_length ) { int id = threadIdx . x + blockDim . x * blockIdx . x ; if ( id >= input_length ) return ; int keyIndex = id % 4 ; char * keyCharPtr = ( ( char * ) & key ) ; char keyChar = keyCharPtr [ keyIndex ] ; possible_plaintext_str_cuda [ id ] = keyChar ^ input_str_cuda [ id ] ; }
__global__ void envejecer_kernel ( int * estado , int * edad , int * pupacion , int * N_mobil , int dia ) { int N = N_mobil [ 0 ] ; int id = blockIdx . x * blockDim . x + threadIdx . x ; if ( id < N ) { if ( dia < 80 || dia > 320 ) { if ( edad [ id ] > pupacion [ id ] ) edad [ id ] ++ ; } else { edad [ id ] ++ ; } } }
__global__ void globalCalculateKernel ( float * c , float * a , float * b ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; int j = blockIdx . y * blockDim . y + threadIdx . y ; c [ i * j ] = sin ( a [ i * j ] ) * sin ( a [ i * j ] ) + cos ( b [ i * j ] ) * cos ( b [ i * j ] ) * cos ( b [ i * j ] ) ; }
__global__ void gpu_matrix_mul ( int * a , int * b , int * c , int N ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; int sum = 0 ; if ( col < N && row < N ) { for ( int i = 0 ; i < N ; i ++ ) { sum += a [ row * N + i ] * b [ i * N + col ] ; } c [ row * N + col ] = sum ; } }
__global__ void grayscale ( unsigned char * input , unsigned char * output , int size ) { unsigned char r , g , b ; int i = threadIdx . x + blockDim . x * blockIdx . x ; if ( i < size ) { r = input [ 3 * i ] ; g = input [ 3 * i + 1 ] ; b = input [ 3 * i + 2 ] ; output [ i ] = ( unsigned char ) ( 0.21 * ( float ) r + 0.71 * ( float ) g + 0.07 * ( float ) b ) ; } }
__global__ void subtractMean ( double * images , const double * meanImage , std :: size_t imageNum , std :: size_t pixelNum ) { std :: size_t col = blockIdx . x * blockDim . x + threadIdx . x ; if ( col >= pixelNum ) { return ; } for ( std :: size_t row = 0 ; row < imageNum ; ++ row ) { images [ row * pixelNum + col ] -= meanImage [ col ] ; if ( images [ row * pixelNum + col ] < 0.0 ) { images [ row * pixelNum + col ] = 0.0 ; } } }
__global__ void kernelMaximum ( float * maxhd , float * maxvd , int start , int size ) { int tx = start + threadIdx . x ; for ( int i = size >> 1 ; i > 0 ; i >>= 1 ) { __syncthreads ( ) ; if ( tx < i ) { if ( maxhd [ tx ] < maxhd [ tx + i ] ) maxhd [ tx ] = maxhd [ tx + i ] ; if ( maxvd [ tx ] < maxvd [ tx + i ] ) maxvd [ tx ] = maxvd [ tx + i ] ; } ; } ; }
__global__ void cuda_SparseMatmul_forward_kernel ( float * a_in , float * b_in , float * c_in , int * indptr , int * indices , int p ) { int i = blockIdx . x ; int k = threadIdx . x ; for ( int jj = indptr [ i ] ; jj < indptr [ i + 1 ] ; jj ++ ) { int j = indices [ jj ] ; c_in [ i * p + k ] += a_in [ jj ] * b_in [ j * p + k ] ; } }
__global__ void vectorMatrixMult ( long int totalPixels , int availablePixels , int outPixelOffset , float * matrix , float * vector , float * out ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = blockDim . x * gridDim . x ; for ( long int i = index ; i < availablePixels ; i += stride ) { float sum = 0.0 ; for ( long int j = 0 ; j < totalPixels ; j ++ ) { sum += matrix [ i * totalPixels + j ] * vector [ j ] ; } out [ i + outPixelOffset ] = sum ; } }
__global__ void convertKinectDisparityInPlace_kernel ( float * d_disparity , int pitch , int width , int height , float depth_scale ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( ( x < width ) & ( y < height ) ) { float * d_in = ( float * ) ( ( char * ) d_disparity + y * pitch ) + x ; * d_in = ( * d_in == 0.0f ) ? 1 : ( - depth_scale / * d_in ) ; } }
__global__ void cuda_SparseMatmul_backward_kernel ( float * a_in , float * b_in , float * c_in , int * indptr , int * indices , int p ) { int i = blockIdx . x ; int k = threadIdx . x ; for ( int jj = indptr [ i ] ; jj < indptr [ i + 1 ] ; jj ++ ) { int j = indices [ jj ] ; b_in [ j * p + k ] += c_in [ i * p + k ] * a_in [ jj ] ; } }
__global__ void subsample_ind_and_labels_GPU ( int * d_ind_sub , const int * d_ind , unsigned int * d_label_sub , const unsigned int * d_label , int n_out , float inv_sub_factor ) { unsigned int ind_out = blockIdx . x * blockDim . x + threadIdx . x ; if ( ind_out < n_out ) { int ind_in = ( int ) floorf ( ( float ) ( ind_out ) * inv_sub_factor ) ; d_ind_sub [ ind_out ] = d_ind [ ind_in ] ; d_label_sub [ ind_out ] = d_label [ ind_in ] ; } }
__global__ void mxm_1d ( double * a , const int m , double * b , const int n , double * c , const int p ) { const int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < m ) { for ( int k = 0 ; k < p ; k ++ ) { double s = 0.0 ; for ( int j = 0 ; j < n ; j ++ ) { s += a [ j * m + i ] * b [ k * n + j ] ; } c [ k * m + i ] = s ; } } }
__global__ void fabsf_clamp_kernel ( int N , float * X , int INCX , float clamp_min , float clamp_max ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) { if ( X [ i * INCX ] >= 0 ) X [ i * INCX ] = fminf ( clamp_max , fmaxf ( clamp_min , X [ i * INCX ] ) ) ; else X [ i * INCX ] = fminf ( - clamp_min , fmaxf ( - clamp_max , X [ i * INCX ] ) ) ; } }
__global__ void gpu_matrix_mult ( int * a , int * b , int * c , int m , int n , int k ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; int sum = 0 ; if ( col < k && row < m ) { for ( int i = 0 ; i < n ; i ++ ) { sum += a [ row * n + i ] * b [ i * k + col ] ; } c [ row * k + col ] = sum ; } }
__global__ void MulMatrixOnGPU ( float * A , float * B , float * C , int nx , int ny ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; int j = threadIdx . y + blockIdx . y * blockDim . y ; int k ; if ( i < nx && j < ny ) { float sum = 0.0 ; for ( k = 0 ; k < nx ; k ++ ) { sum += A [ i * nx + k ] * B [ k * nx + j ] ; } C [ i * nx + j ] = sum ; } }
__global__ void MatrixMulKernel ( float * d_M , float * d_N , float * d_P , int width ) { int Row = blockIdx . y * blockDim . y + threadIdx . y ; int Col = blockIdx . x * blockDim . x + threadIdx . x ; if ( ( Row < width ) && ( Col < width ) ) { float Pvalue = 0 ; for ( int i = 0 ; i < width ; ++ i ) { Pvalue += d_M [ Row * width + i ] * d_N [ i * width + Col ] ; } d_P [ Row * width + Col ] = Pvalue ; } }
__global__ void mmul ( const float * A , const float * B , float * C , int r1 , int c1 , int r2 , int c2 ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; int idy = threadIdx . y + blockDim . y * blockIdx . y ; if ( ( idx < c2 ) && ( idy < c1 ) ) { float temp = 0 ; for ( int i = 0 ; i < c1 ; i ++ ) temp += A [ idy * c1 + i ] * B [ i * c2 + idx ] ; C [ idy * c2 + idx ] = temp ; } }
__global__ void Kernel_Dot_reduction2 ( float * dev_c , float * reduction , int r , const int c , const int n , int size_block ) { unsigned int i = blockDim . x * blockIdx . x + threadIdx . x ; unsigned int j = blockDim . y * blockIdx . y + threadIdx . y ; if ( i >= r || j >= c ) return ; float temp = 0 ; for ( int k = 0 ; k < size_block ; k ++ ) { temp += reduction [ i * ( c * size_block ) + j * ( size_block ) + k ] ; } dev_c [ i * c + j ] = temp ; }
__global__ void Forwardsub ( double * RES , double * LS , double * LW , double * LPR , int NI , int NJ , int Start , int J , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { int IJ = ( ( Start + i ) * NI ) + ( J - ( Start + i ) ) ; RES [ IJ ] = ( RES [ IJ ] - LS [ IJ ] * RES [ IJ - 1 ] - LW [ IJ ] * RES [ IJ - NJ ] ) * LPR [ IJ ] ; } }
__global__ void cuda_rows_dc_offset_remove_layer_kernel ( float * output , float * input , unsigned int width , unsigned int height , unsigned int depth ) { unsigned int column = threadIdx . x + blockIdx . x * blockDim . x ; unsigned int row = threadIdx . y + blockIdx . y * blockDim . y ; unsigned int channel = threadIdx . z + blockIdx . z * blockDim . z ; if ( channel < depth ) if ( row < height ) if ( column < ( width - 1 ) ) { unsigned int idx = ( channel * height + row ) * width + column ; output [ idx ] = input [ idx ] - input [ idx + 1 ] ; } }
__global__ void cuda_cross_correlate ( float * Isg , float * Iss , float * sp , float * gp , int npml , int nnz , int nnx ) { int i1 = threadIdx . x + blockDim . x * blockIdx . x ; int i2 = threadIdx . y + blockDim . y * blockIdx . y ; int id = i1 + i2 * nnz ; if ( i1 >= npml && i1 < nnz - npml && i2 >= npml && i2 < nnx - npml ) { float ps = sp [ id ] ; float pg = gp [ id ] ; Isg [ id ] += ps * pg ; Iss [ id ] += ps * ps ; } }
__global__ void colorConvert ( unsigned char * grayImage , unsigned char * colorImage , int rows , int columns ) { int column = blockIdx . x * blockDim . x + threadIdx . x ; int row = blockIdx . y * blockDim . y + threadIdx . y ; if ( ( column < columns ) && ( row < rows ) ) { int offset = ( column ) + ( columns * row ) ; unsigned char grayValue = 0.07 * colorImage [ offset * 3 ] + 0.71 * colorImage [ offset * 3 + 1 ] + 0.21 * colorImage [ offset * 3 + 2 ] ; grayImage [ offset ] = grayValue ; } }
__global__ void init_image_array_GPU ( unsigned long long int * image , int pixels_per_image ) { int my_pixel = threadIdx . x + blockIdx . x * blockDim . x ; if ( my_pixel < pixels_per_image ) { image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; } }
__global__ void diffusion ( double * x0 , double * x1 , int nx , int ny , double dt ) { int i = threadIdx . x + blockDim . x * blockIdx . x + 1 ; int j = threadIdx . y + blockDim . y * blockIdx . y + 1 ; if ( i < nx - 1 && j < ny - 1 ) { int pos = nx * j + i ; x1 [ pos ] = x0 [ pos ] + dt * ( -4. * x0 [ pos ] + x0 [ pos - 1 ] + x0 [ pos + 1 ] + x0 [ pos - nx ] + x0 [ pos + nx ] ) ; } }
__global__ void compute_b_minus_Rx ( double * out , double * x , double * b , double * cotans , int * neighbors , int meshStride , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = gridDim . x * blockDim . x ; for ( int i = index ; i < n ; i += stride ) { out [ i ] = b [ i ] ; for ( int iN = 0 ; iN < meshStride ; ++ iN ) { int neighbor = neighbors [ i * meshStride + iN ] ; double weight = cotans [ i * meshStride + iN ] ; out [ i ] += weight * x [ neighbor ] ; } } }
__global__ void binarize_weights_kernel ( float * weights , int n , int size , float * binary ) { int f = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( f >= n ) return ; int i = 0 ; float mean = 0 ; for ( i = 0 ; i < size ; ++ i ) { mean += abs ( weights [ f * size + i ] ) ; } mean = mean / size ; for ( i = 0 ; i < size ; ++ i ) { binary [ f * size + i ] = ( weights [ f * size + i ] > 0 ) ? mean : - mean ; } }
__global__ void gather_points_kernel ( int b , int c , int n , int m , const float * __restrict__ points , const int * __restrict__ idx , float * __restrict__ out ) { for ( int i = blockIdx . x ; i < b ; i += gridDim . x ) { for ( int l = blockIdx . y ; l < c ; l += gridDim . y ) { for ( int j = threadIdx . x ; j < m ; j += blockDim . x ) { int a = idx [ i * m + j ] ; out [ ( i * c + l ) * m + j ] = points [ ( i * c + l ) * n + a ] ; } } } }
__global__ void gpu_matrix_mult ( int left_rows , int shared_dimensions , int right_columns , float * left , float * right , float * result ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int column = blockIdx . x * blockDim . x + threadIdx . x ; if ( row < left_rows && column < right_columns ) { int index = row * right_columns + column ; result [ index ] = 0 ; int cell ; for ( cell = 0 ; cell < shared_dimensions ; cell ++ ) { result [ index ] += left [ row * shared_dimensions + cell ] * right [ cell * right_columns + column ] ; } } }
__global__ void matrixMultiplication ( int * dev_a , int * dev_b , int * dev_c , int row_a , int col_a , int col_b ) { int row = threadIdx . y + blockIdx . y * blockDim . y ; int col = threadIdx . x + blockIdx . x * blockDim . x ; int ret = 0 ; if ( row < row_a && col < col_b ) { for ( int i = 0 ; i < col_a ; ++ i ) { ret += dev_a [ row * col_a + i ] * dev_b [ i * col_b + col ] ; } dev_c [ row * col_b + col ] = ret ; } }
__global__ void Backwardsub ( double * U , double * RES , double * UN , double * UE , double * LPR , int NI , int NJ , int End , int J , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { int IJ = ( ( End - i ) * NI ) + ( J - ( End - i ) ) ; RES [ IJ ] = RES [ IJ ] - UN [ IJ ] * RES [ IJ + 1 ] - UE [ IJ ] * RES [ IJ + NJ ] ; U [ IJ ] = U [ IJ ] + RES [ IJ ] ; } }
__global__ void convolution_gpu_1d_naive ( float * input , float * mask , float * output , int array_size , int mask_size ) { int gid = blockIdx . x * blockDim . x + threadIdx . x ; int MASK_RADIUS = mask_size / 2 ; int ELEMENT_INDEX = 0 ; float temp = 0.0f ; if ( gid < array_size ) { for ( int j = 0 ; j < mask_size ; j ++ ) { ELEMENT_INDEX = gid - MASK_RADIUS + j ; if ( ! ( ELEMENT_INDEX < 0 || ELEMENT_INDEX > ( array_size - 1 ) ) ) { temp += input [ ELEMENT_INDEX ] * mask [ j ] ; } } output [ gid ] = temp ; } }
__global__ void getRho_cuda ( const double * psi , const double * occNo , double * rho ) { extern __shared__ double dcopy [ ] ; dcopy [ threadIdx . x ] = occNo [ threadIdx . x ] * psi [ threadIdx . x ] * psi [ threadIdx . x ] ; __syncthreads ( ) ; for ( int tc = blockDim . x , stepSize = 1 ; tc > 0 ; tc >>= 1 , stepSize <<= 1 ) { int pa = threadIdx . x * stepSize ; int pb = pa + stepSize ; if ( pb < blockDim . x ) { dcopy [ pa ] += dcopy [ pb ] ; } } if ( threadIdx . x == 0 ) { * rho = dcopy [ 0 ] ; } }
__global__ void colLog2SumExp2Kernel ( const double * mat , double * buf , int m , int n ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; if ( j < n ) { double maximum = mat [ j ] ; for ( int i = 1 ; i < m ; i ++ ) { if ( mat [ i * n + j ] > maximum ) { maximum = mat [ i * n + j ] ; } } double res = 0.0 ; for ( int i = 0 ; i < m ; i ++ ) { res += mat [ i * n + j ] - maximum ; } buf [ j ] = res + maximum ; } }
__global__ void bitPrune ( unsigned char * out , float * in , int frontPrune , int outputlength , int inputLength , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= n ) return ; int batch = i / outputlength ; int indexInBatch = i % outputlength ; int batchInJump = batch * inputLength ; int indexOutBatch = i % outputlength ; int batchOutJump = batch * outputlength ; int frontJump = frontPrune ; out [ batchOutJump + indexOutBatch ] = ( char ) ( in [ batchInJump + frontJump + indexInBatch ] > 0 ) ; }
__global__ void residual ( double * out , double * x , double * b , double * cotans , int * neighbors , double * diag , int meshStride , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = gridDim . x * blockDim . x ; for ( int i = index ; i < n ; i += stride ) { out [ i ] = diag [ i ] * x [ i ] - b [ i ] ; for ( int iN = 0 ; iN < meshStride ; ++ iN ) { int neighbor = neighbors [ i * meshStride + iN ] ; double weight = cotans [ i * meshStride + iN ] ; out [ i ] -= weight * x [ neighbor ] ; } } }
__global__ void forward_avgpool_layer_kernel ( int n , int w , int h , int c , float * input , float * output ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id >= n ) return ; int k = id % c ; id /= c ; int b = id ; int i ; int out_index = ( k + c * b ) ; output [ out_index ] = 0 ; for ( i = 0 ; i < w * h ; ++ i ) { int in_index = i + h * w * ( k + b * c ) ; output [ out_index ] += input [ in_index ] ; } output [ out_index ] /= w * h ; }
__global__ void kernel_columns ( const float * filter , const float * buffer , float * output , int imageW , int imageH , int filterR ) { int idx_x = threadIdx . x + blockDim . x * blockIdx . x ; int idx_y = threadIdx . y + blockDim . y * blockIdx . y ; int grid_width = gridDim . x * blockDim . x ; int idx = grid_width * idx_y + idx_x ; float sum = 0 ; int k ; for ( k = - filterR ; k <= filterR ; k ++ ) { int d = idx_y + k ; if ( d >= 0 && d < imageH ) { sum += buffer [ d * imageW + idx_x ] * filter [ filterR - k ] ; } } output [ idx ] = sum ; }
__global__ void gpuMatrMultD ( float * Ad , float * Bd , float * Cd , int rowsA , int colsA , int colsB ) { int bIndx = blockIdx . x ; int bIndy = blockIdx . y ; int tIndx = threadIdx . x ; int tIndy = threadIdx . y ; Cd [ ( blockDim . x * bIndx + tIndx ) * colsB + blockDim . y * bIndy + tIndy ] = 0 ; for ( int k = 0 ; k < colsA ; ++ k ) { Cd [ ( blockDim . x * bIndx + tIndx ) * colsB + blockDim . y * bIndy + tIndy ] += Ad [ ( blockDim . x * bIndx + tIndx ) * colsA + k ] * Bd [ k * colsB + blockDim . y * bIndy + tIndy ] ; } }
__global__ void add_sources_d ( const float * const model , float * wfp , const float * const source_amplitude , const int * const sources_z , const int * const sources_x , const int nz , const int nx , const int nt , const int ns , const int it ) { int x = threadIdx . x ; int b = blockIdx . x ; int i = sources_z [ b * ns + x ] * nx + sources_x [ b * ns + x ] ; int ib = b * nz * nx + i ; wfp [ ib ] += source_amplitude [ b * ns * nt + x * nt + it ] * model [ i ] ; }
__global__ void variance_kernel ( float * x , float * mean , int batch , int filters , int spatial , float * variance ) { float scale = 1.f / ( batch * spatial - 1 ) ; int j , k ; int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i >= filters ) return ; variance [ i ] = 0 ; for ( j = 0 ; j < batch ; ++ j ) { for ( k = 0 ; k < spatial ; ++ k ) { int index = j * filters * spatial + i * spatial + k ; variance [ i ] += powf ( ( x [ index ] - mean [ i ] ) , 2 ) ; } } variance [ i ] *= scale ; }
__global__ void grad_y ( const float * u , float * grad , long depth , long rows , long cols ) { unsigned long x = threadIdx . x + blockIdx . x * blockDim . x ; unsigned long y = threadIdx . y + blockIdx . y * blockDim . y ; unsigned long z = threadIdx . z + blockIdx . z * blockDim . z ; if ( x >= cols || y >= rows || z >= depth ) return ; unsigned long size2d = rows * cols ; unsigned long long idx = z * size2d + y * cols + x ; float uidx = u [ idx ] ; if ( y - 1 >= 0 && y < rows ) { grad [ idx ] = ( uidx - u [ z * size2d + ( y - 1 ) * cols + x ] ) ; } }
__global__ void grad_x ( const float * u , float * grad , long depth , long rows , long cols ) { unsigned long x = threadIdx . x + blockIdx . x * blockDim . x ; unsigned long y = threadIdx . y + blockIdx . y * blockDim . y ; unsigned long z = threadIdx . z + blockIdx . z * blockDim . z ; if ( x >= cols || y >= rows || z >= depth ) return ; unsigned long size2d = rows * cols ; unsigned long long idx = z * size2d + y * cols + x ; float uidx = u [ idx ] ; if ( x - 1 >= 0 && x < cols ) { grad [ idx ] = ( uidx - u [ z * size2d + y * cols + ( x - 1 ) ] ) ; } }
__global__ void cuda_GraphSum_forward_kernel ( float * d_in_data , float * d_out_data , int * d_indptr , int * d_indices , int dim , int numNodes ) { int src = blockIdx . x ; int j = threadIdx . x ; int ptr_src_0 = d_indptr [ src ] ; int ptr_stc_1 = d_indptr [ src + 1 ] ; for ( int i = ptr_src_0 ; i < ptr_stc_1 ; i ++ ) { int dst = d_indices [ i ] ; float coef = 1.0 / sqrtf ( ( ptr_stc_1 - ptr_src_0 ) * ( d_indptr [ dst + 1 ] - d_indptr [ dst ] ) ) ; d_out_data [ src * dim + j ] += coef * d_in_data [ dst * dim + j ] ; } }
__global__ void apply_grayscale ( unsigned char * grayimg , const unsigned char * image , int width , int height ) { unsigned int x = blockIdx . x * blockDim . x + threadIdx . x ; unsigned int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( x < width && y < height ) { const unsigned char R = image [ ( y * width + x ) * 3 + 0 ] ; const unsigned char G = image [ ( y * width + x ) * 3 + 1 ] ; const unsigned char B = image [ ( y * width + x ) * 3 + 2 ] ; unsigned char gray = ( 307 * R + 604 * G + 113 * B ) >> 10 ; grayimg [ y * width + x ] = gray ; } }
__global__ void getOffsetBox ( const int * clsIndex , const float * max_coordinate , float * offset , int dims , int batchSize , const float * before_nms_boxes ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } int numPerbatch = dims ; for ( int i = 0 ; i < batchSize ; i ++ ) { if ( before_nms_boxes [ i * dims * 4 + tid * 4 ] == ( -1 ) ) { offset [ i * numPerbatch + tid ] = 0 ; } else { offset [ i * numPerbatch + tid ] = clsIndex [ i * numPerbatch + tid ] * ( max_coordinate [ i * dims * 4 ] + 1 ) ; } } }
__global__ void sgemm_kernelGPU ( const float * host_inputArray1 , const float * host_inputArray2 , float * host_inputArray3 , int M , int N , int K , float alpha , float beta ) { int column = blockIdx . x * blockDim . x + threadIdx . x ; int row = blockIdx . y * blockDim . y + threadIdx . y ; float element_c = 0.f ; for ( int eachElement = 0 ; eachElement < K ; eachElement ++ ) element_c += host_inputArray1 [ row * K + eachElement ] * host_inputArray2 [ eachElement * K + column ] ; host_inputArray3 [ row * N + column ] = alpha * element_c + beta * host_inputArray3 [ row * N + column ] ; }
__global__ void cuda_GraphSum_backward_kernel ( float * d_in_grad , float * d_out_grad , int * d_indptr , int * d_indices , int dim , int numNodes ) { int src = blockIdx . x ; int j = threadIdx . x ; int ptr_src_0 = d_indptr [ src ] ; int ptr_stc_1 = d_indptr [ src + 1 ] ; #pragma unroll ENDCOM for ( int i = ptr_src_0 ; i < ptr_stc_1 ; i ++ ) { int dst = d_indices [ i ] ; float coef = 1.0 / sqrtf ( ( ptr_stc_1 - ptr_src_0 ) * ( d_indptr [ dst + 1 ] - d_indptr [ dst ] ) ) ; d_in_grad [ src * dim + j ] += coef * d_out_grad [ dst * dim + j ] ; } }
__global__ void CDFfunction ( float * median , float * stdvLogNormalFrame , float * MeanLogNormalFrame , unsigned char * currentFrame , int pixelsPerFrame ) { int pixel = threadIdx . x + blockIdx . x * blockDim . x ; if ( pixel < pixelsPerFrame ) { float newvalue ; float x = currentFrame [ pixel ] ; newvalue = - ( ( logf ( x ) - median [ pixel ] ) - MeanLogNormalFrame [ pixel ] ) / ( sqrtf ( 2 ) * stdvLogNormalFrame [ pixel ] ) ; float summ = 0.5f + 0.5f * erff ( newvalue ) ; if ( summ >= 0.3 ) { currentFrame [ pixel ] = ( unsigned char ) 255 ; } else { currentFrame [ pixel ] = ( unsigned char ) 0 ; } } }
__global__ void matrixmul ( float * Md , float * Nd , float * Pd , float width , float width_blk , float height_blk , float width_M , float width_N , float height_M , int m , int n ) { int bx = blockIdx . x ; int by = blockIdx . y ; int tx = threadIdx . x ; int ty = threadIdx . y ; int Row = by * width_blk + ty ; int Col = bx * height_blk + tx ; float pValue = 0 ; if ( Col < ( int ) width_N && Row < ( int ) height_M ) { for ( int i = 0 ; i < width ; i ++ ) { float Melement = Md [ Row * ( int ) width_M + i ] ; float Nelement = Nd [ i * ( int ) width_N + Col ] ; pValue += Melement * Nelement ; } Pd [ Row * ( int ) width_N + Col ] = pValue ; } }
__global__ void softmax_kernel ( float * input , int n , int batch , int batch_offset , int groups , int group_offset , int stride , float temp , float * output ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id >= batch * groups ) return ; int b = id / groups ; int g = id % groups ; int i ; float sum = 0 ; float largest = - INFINITY ; for ( i = 0 ; i < n ; ++ i ) { int val = ( input + b * batch_offset + g * group_offset ) [ i * stride ] ; largest = ( val > largest ) ? val : largest ; } for ( i = 0 ; i < n ; ++ i ) { float e = expf ( ( input + b * batch_offset + g * group_offset ) [ i * stride ] / temp - largest / temp ) ; sum += e ; ( output + b * batch_offset + g * group_offset ) [ i * stride ] = e ; } for ( i = 0 ; i < n ; ++ i ) { ( output + b * batch_offset + g * group_offset ) [ i * stride ] /= sum ; } }
__global__ void normalizacion ( float * image_c , int bands , long int image_size , float * normM_c , float * normM1_c ) { long int j , i ; float norm_val = 0 , aux = 0 , pixel = 0 ; i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < image_size ) { for ( j = 0 ; j < bands ; j ++ ) { norm_val += image_c [ j * image_size + i ] ; } norm_val = 1.0 / ( norm_val + 1.0e-16 ) ; for ( j = 0 ; j < bands ; j ++ ) { pixel = image_c [ j * image_size + i ] * norm_val ; image_c [ j * image_size + i ] = pixel ; aux += pixel * pixel ; } normM_c [ i ] = aux ; normM1_c [ i ] = aux ; } }
__global__ void permuteData ( const float * input , float * output , int num , int devideNum , int featureSize , int priorNum , int batchSize ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= num ) { return ; } int numPerbatch = num * devideNum * priorNum ; for ( int s = 0 ; s < batchSize ; s ++ ) { for ( int i = 0 ; i < priorNum ; i ++ ) { for ( int j = 0 ; j < devideNum ; j ++ ) { output [ s * numPerbatch + tid * priorNum * devideNum + i * devideNum + j ] = input [ s * numPerbatch + ( i * devideNum * featureSize ) + ( j * featureSize ) + tid ] ; } } } }
__global__ void cudaSimpleCorrelator ( float * xi , float * xq , float * sr , float * si , int sLength , float * L , int uLength ) { int u = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( u >= uLength ) return ; float real = 0 ; float imag = 0 ; float a , b , c , d ; for ( int n = u ; n < u + sLength ; n ++ ) { a = xi [ n ] ; b = xq [ n ] ; c = sr [ n - u ] ; d = si [ n - u ] * ( -1 ) ; real += ( a * c ) - ( b * d ) ; imag += ( a * d ) + ( b * c ) ; } L [ u ] = sqrt ( real * real + imag * imag ) ; }
__global__ void convertKinectDisparityToRegularDisparity_kernel ( float * d_regularDisparity , int d_regularDisparityPitch , const float * d_KinectDisparity , int d_KinectDisparityPitch , int width , int height ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( ( x < width ) & ( y < height ) ) { float d_in = * ( ( float * ) ( ( char * ) d_KinectDisparity + y * d_KinectDisparityPitch ) + x ) ; float d_out = ( d_in == 0.0f ) ? 1 : - d_in ; * ( ( float * ) ( ( char * ) d_regularDisparity + y * d_regularDisparityPitch ) + x ) = d_out ; } }
__global__ void runFilterCuda ( float * I , float * Q , int samplesLength , float * filter , int filterLength , float * filtered_I , float * filtered_Q , int convLength ) { int sampleIndex = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( sampleIndex >= convLength ) return ; int index ; float sumI , sumQ ; sumI = 0 ; sumQ = 0 ; for ( int j = sampleIndex - filterLength + 1 ; j <= sampleIndex ; j ++ ) { index = sampleIndex - j ; if ( ( j < samplesLength ) && ( j >= 0 ) ) { sumI += filter [ index ] * I [ j ] ; sumQ += filter [ index ] * Q [ j ] ; } } filtered_I [ sampleIndex ] = sumI ; filtered_Q [ sampleIndex ] = sumQ ; }
__global__ void l2normalize_kernel ( int N , float * x , float * dx , int batch , int filters , int spatial ) { int index = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( index >= N ) return ; int b = index / spatial ; int i = index % spatial ; int f ; float sum = 0 ; for ( f = 0 ; f < filters ; ++ f ) { int index = b * filters * spatial + f * spatial + i ; sum += powf ( x [ index ] , 2 ) ; } sum = sqrtf ( sum ) ; if ( sum == 0 ) sum = 1 ; for ( f = 0 ; f < filters ; ++ f ) { int index = b * filters * spatial + f * spatial + i ; x [ index ] /= sum ; dx [ index ] = ( 1 - x [ index ] ) / sum ; } }
__global__ void distanceMatCalc ( long int totalPixels , int availablePixels , int outPixelOffset , int patchSize , float * distMat , float * data , float filtSig ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = blockDim . x * gridDim . x ; for ( long int i = index ; i < availablePixels * totalPixels ; i += stride ) { int data_i = i / totalPixels + outPixelOffset ; int data_j = i % totalPixels ; float tmp = 0.0 ; if ( data_i != data_j ) { for ( int elem = 0 ; elem < patchSize * patchSize ; elem ++ ) { float diff = ( data [ data_i * patchSize * patchSize + elem ] - data [ data_j * patchSize * patchSize + elem ] ) ; tmp += diff * diff ; } tmp = exp ( - tmp / ( filtSig ) ) ; } distMat [ i ] = tmp ; } }
__global__ void shortcut_kernel ( int size , int minw , int minh , int minc , int stride , int sample , int batch , int w1 , int h1 , int c1 , float * add , int w2 , int h2 , int c2 , float * out ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id >= size ) return ; int i = id % minw ; id /= minw ; int j = id % minh ; id /= minh ; int k = id % minc ; id /= minc ; int b = id % batch ; int out_index = i * sample + w2 * ( j * sample + h2 * ( k + c2 * b ) ) ; int add_index = i * stride + w1 * ( j * stride + h1 * ( k + c1 * b ) ) ; out [ out_index ] += add [ add_index ] ; }
__global__ void dot_kernel ( float * output , float scale , int batch , int n , int size , float * delta ) { int index = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; int f1 = index / n ; int f2 = index % n ; if ( f2 <= f1 ) return ; float sum = 0 ; float norm1 = 0 ; float norm2 = 0 ; int b , i ; for ( b = 0 ; b < batch ; ++ b ) { for ( i = 0 ; i < size ; ++ i ) { int i1 = b * size * n + f1 * size + i ; int i2 = b * size * n + f2 * size + i ; sum += output [ i1 ] * output [ i2 ] ; norm1 += output [ i1 ] * output [ i1 ] ; norm2 += output [ i2 ] * output [ i2 ] ; } } norm1 = sqrt ( norm1 ) ; norm2 = sqrt ( norm2 ) ; float norm = norm1 * norm2 ; sum = sum / norm ; for ( b = 0 ; b < batch ; ++ b ) { for ( i = 0 ; i < size ; ++ i ) { int i1 = b * size * n + f1 * size + i ; int i2 = b * size * n + f2 * size + i ; delta [ i1 ] += - scale * sum * output [ i2 ] / norm ; delta [ i2 ] += - scale * sum * output [ i1 ] / norm ; } } }
__global__ void k_adam_kernel ( float * m , float * v , float * w , const float * d , int max_size , float beta1 , float beta2 , float beta1_tpower , float beta2_tpower , float learning_rate ) { const float eps = 1e-8 ; for ( int i = blockIdx . x * blockDim . x + threadIdx . x ; i < max_size ; i += blockDim . x * gridDim . x ) { float d_temp = d [ i ] ; m [ i ] = m [ i ] * beta1 + d_temp * ( 1 - beta1 ) ; v [ i ] = v [ i ] * beta2 + d_temp * d_temp * ( 1 - beta2 ) ; float m_hat = m [ i ] / ( 1 - beta1_tpower ) ; float v_hat = __fsqrt_rn ( v [ i ] / ( 1 - beta2_tpower ) ) + eps ; w [ i ] += ( m_hat / v_hat ) * ( - learning_rate ) ; } }
__global__ void ConvLayerForward_Kernel ( int C , int W_grid , int K , float * X , float * W , float * Y ) { int n , m , h , w , c , p , q ; n = blockIdx . x ; m = blockIdx . y ; h = blockIdx . z / W_grid + threadIdx . y ; w = blockIdx . z % W_grid + threadIdx . x ; float acc = 0 ; for ( c = 0 ; c < C ; c ++ ) { for ( p = 0 ; p < K ; p ++ ) for ( q = 0 ; q < K ; q ++ ) acc = acc + X [ n , c , h + p , w + q ] * W [ m , c , p , q ] ; } Y [ n , m , h , w ] = acc ; }
__global__ void opL23 ( float * vec , float * vec1 , long depth , long rows , long cols ) { unsigned long x = threadIdx . x + blockIdx . x * blockDim . x ; unsigned long y = threadIdx . y + blockIdx . y * blockDim . y ; unsigned long z = threadIdx . z + blockIdx . z * blockDim . z ; unsigned long long i = z * rows * cols + y * cols + x ; unsigned long long j = z * rows * cols + y * cols ; unsigned long size2d = cols ; unsigned long size3d = depth * rows * cols + rows * cols + cols ; if ( x >= cols || y >= rows || z >= depth ) return ; if ( i + cols + 1 >= size3d ) return ; vec [ i + cols ] = 0.5 * ( vec1 [ i + cols ] + vec1 [ i ] ) ; if ( j + 1 >= size2d ) return ; vec [ j ] = 0.5 * ( vec1 [ j ] ) ; }
__global__ void upsample_kernel ( size_t N , float * x , int w , int h , int c , int batch , int stride , int forward , float scale , float * out ) { size_t i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i >= N ) return ; int out_index = i ; int out_w = i % ( w * stride ) ; i = i / ( w * stride ) ; int out_h = i % ( h * stride ) ; i = i / ( h * stride ) ; int out_c = i % c ; i = i / c ; int b = i % batch ; int in_w = out_w / stride ; int in_h = out_h / stride ; int in_c = out_c ; int in_index = b * w * h * c + in_c * w * h + in_h * w + in_w ; if ( forward ) out [ out_index ] += scale * x [ in_index ] ; else atomicAdd ( x + in_index , scale * out [ out_index ] ) ; }
__global__ void rgb2yuv_kernel ( int img_size , unsigned char * gpu_img_in_r , unsigned char * gpu_img_in_g , unsigned char * gpu_img_in_b , unsigned char * gpu_img_out_y , unsigned char * gpu_img_out_u , unsigned char * gpu_img_out_v ) { unsigned char r , g , b ; int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < img_size ) { r = gpu_img_in_r [ index ] ; g = gpu_img_in_g [ index ] ; b = gpu_img_in_b [ index ] ; gpu_img_out_y [ index ] = ( unsigned char ) ( 0.299 * r + 0.587 * g + 0.114 * b ) ; gpu_img_out_u [ index ] = ( unsigned char ) ( -0.169 * r - 0.331 * g + 0.499 * b + 128 ) ; gpu_img_out_v [ index ] = ( unsigned char ) ( 0.499 * r - 0.418 * g - 0.0813 * b + 128 ) ; } }
__global__ void getDRho_cuda ( const double * psi , const double * dpsi , const double * occNo , double * drho ) { extern __shared__ double dcopy [ ] ; unsigned int idx = blockIdx . x + gridDim . x * threadIdx . x ; dcopy [ threadIdx . x ] = 2 * occNo [ threadIdx . x ] * psi [ threadIdx . x ] * dpsi [ idx ] ; __syncthreads ( ) ; for ( int tc = blockDim . x , stepSize = 1 ; tc > 0 ; tc >>= 1 , stepSize <<= 1 ) { int pa = threadIdx . x * stepSize ; int pb = pa + stepSize ; if ( pb < blockDim . x ) dcopy [ pa ] += dcopy [ pb ] ; } if ( threadIdx . x == 0 ) { drho [ blockIdx . x ] = dcopy [ 0 ] ; } }
__global__ void opL12 ( float * vec , float * vec1 , long depth , long rows , long cols ) { unsigned long x = threadIdx . x + blockIdx . x * blockDim . x ; unsigned long y = threadIdx . y + blockIdx . y * blockDim . y ; unsigned long z = threadIdx . z + blockIdx . z * blockDim . z ; unsigned long long i = z * rows * cols + y * cols + x ; unsigned long long j = z * rows * cols + y * cols ; unsigned long size2d = cols ; unsigned long size3d = depth * rows * cols + rows * cols + cols ; if ( x >= cols || y >= rows || z >= depth ) return ; if ( i + cols + 1 >= size3d ) return ; vec [ i + 1 ] = 0.25 * ( vec1 [ i + 1 ] + vec1 [ i ] + vec1 [ i + cols + 1 ] + vec1 [ i + cols ] ) ; if ( j + 1 >= size2d ) return ; vec [ j ] = 0.25 * ( vec1 [ j ] + vec1 [ j + cols ] ) ; }
__global__ void cudaBYUSimplified ( float * xi , float * xq , float * sr , float * si , int N , int Lq , float * L ) { int u = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( u >= N ) return ; float uSum = 0 ; float r_i , r_q , q_i , q_q ; float realPart , imagPart ; for ( int k = 0 ; k <= 7 ; k ++ ) { realPart = 0 ; imagPart = 0 ; for ( int l = 0 ; l < Lq ; l ++ ) { r_i = xi [ u + k * Lq + l ] ; r_q = xq [ u + k * Lq + l ] ; q_i = sr [ l ] ; q_q = si [ l ] * ( -1 ) ; realPart += ( r_i * q_i ) - ( r_q * q_q ) ; imagPart += ( r_i * q_q ) + ( r_q * q_i ) ; } uSum += ( realPart * realPart ) + ( imagPart * imagPart ) ; } L [ u ] = uSum ; }
__global__ void shortcut_kernel ( int size , int minw , int minh , int minc , int stride , int sample , int batch , int w1 , int h1 , int c1 , float * add , int w2 , int h2 , int c2 , float s1 , float s2 , float * out ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id >= size ) return ; int i = id % minw ; id /= minw ; int j = id % minh ; id /= minh ; int k = id % minc ; id /= minc ; int b = id % batch ; int out_index = i * sample + w2 * ( j * sample + h2 * ( k + c2 * b ) ) ; int add_index = i * stride + w1 * ( j * stride + h1 * ( k + c1 * b ) ) ; out [ out_index ] = s1 * out [ out_index ] + s2 * add [ add_index ] ; }
__global__ void get_before_nms_data ( const float * boxes , const float * scores , const int * labels , const int * index , float * boxes_out , float * scores_out , int * labels_out , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( index [ tid ] == 0 ) { boxes_out [ tid * 4 + 0 ] = -1 ; boxes_out [ tid * 4 + 1 ] = -1 ; boxes_out [ tid * 4 + 2 ] = -1 ; boxes_out [ tid * 4 + 3 ] = -1 ; scores_out [ tid ] = -1 ; labels_out [ tid ] = -1 ; } else { boxes_out [ tid * 4 + 0 ] = boxes [ tid * 4 + 0 ] ; boxes_out [ tid * 4 + 1 ] = boxes [ tid * 4 + 1 ] ; boxes_out [ tid * 4 + 2 ] = boxes [ tid * 4 + 2 ] ; boxes_out [ tid * 4 + 3 ] = boxes [ tid * 4 + 3 ] ; scores_out [ tid ] = scores [ tid ] ; labels_out [ tid ] = labels [ tid ] ; } }
__global__ void im2col_gpu_kernel ( const int n , const float * data_im , const int height , const int width , const int ksize , const int pad , const int stride , const int height_col , const int width_col , float * data_col ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; for ( ; index < n ; index += blockDim . x * gridDim . x ) { int w_out = index % width_col ; int h_index = index / width_col ; int h_out = h_index % height_col ; int channel_in = h_index / height_col ; int channel_out = channel_in * ksize * ksize ; int h_in = h_out * stride - pad ; int w_in = w_out * stride - pad ; float * data_col_ptr = data_col ; data_col_ptr += ( channel_out * height_col + h_out ) * width_col + w_out ; const float * data_im_ptr = data_im ; data_im_ptr += ( channel_in * height + h_in ) * width + w_in ; for ( int i = 0 ; i < ksize ; ++ i ) { for ( int j = 0 ; j < ksize ; ++ j ) { int h = h_in + i ; int w = w_in + j ; * data_col_ptr = ( h >= 0 && w >= 0 && h < height && w < width ) ? data_im_ptr [ i * width + j ] : 0 ; data_col_ptr += height_col * width_col ; } } } }
__global__ void getTopkNum ( const float * inputScore , const int * inputIndex , float * outputScore , int * outputIndex , float threshold , const int dims , int * anchorIndex , int * classIndex , const int classNum , int batchSize , int totalScoreNum ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } for ( int i = 0 ; i < batchSize ; i ++ ) { if ( inputScore [ i * totalScoreNum + tid ] >= threshold ) { outputScore [ i * dims + tid ] = inputScore [ i * totalScoreNum + tid ] ; outputIndex [ i * dims + tid ] = inputIndex [ i * totalScoreNum + tid ] ; anchorIndex [ i * dims + tid ] = outputIndex [ i * dims + tid ] / classNum ; classIndex [ i * dims + tid ] = outputIndex [ i * dims + tid ] % classNum ; } else { outputScore [ i * dims + tid ] = 0.0f ; outputIndex [ i * dims + tid ] = -1 ; anchorIndex [ i * dims + tid ] = -1 ; classIndex [ i * dims + tid ] = -1 ; } } }
__global__ void fractal ( const int width , const int frames , unsigned char * const pic ) { const long i = threadIdx . x + blockIdx . x * ( long ) blockDim . x ; if ( i > width * width * frames ) { return ; } const float Delta = 0.00304f ; const float xMid = -0.055846456f ; const float yMid = -0.668311119f ; const int frame = i / ( width * width ) ; float delta = Delta * powf ( 0.975f , frame ) ; const int col = i % width ; const float xMin = xMid - delta ; const float yMin = yMid - delta ; const float dw = 2.0f * delta / width ; const int row = ( i / width ) % width ; const float cy = yMin + row * dw ; const float cx = xMin + col * dw ; float x = cx ; float y = cy ; float x2 , y2 ; int count = 256 ; do { x2 = x * x ; y2 = y * y ; y = 2.0 * x * y + cy ; x = x2 - y2 + cx ; count -- ; } while ( ( count > 0 ) && ( ( x2 + y2 ) <= 5.0 ) ) ; pic [ frame * width * width + row * width + col ] = ( unsigned char ) count ; }
__global__ void bit8Channels ( unsigned char * out , unsigned char * in , int channel , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= n ) return ; int firstIndexToGrab = i * 8 ; unsigned char bit0 = ( in [ firstIndexToGrab + 0 ] & 0x01 ) << 0 ; unsigned char bit1 = ( in [ firstIndexToGrab + 1 ] & 0x01 ) << 1 ; unsigned char bit2 = ( in [ firstIndexToGrab + 2 ] & 0x01 ) << 2 ; unsigned char bit3 = ( in [ firstIndexToGrab + 3 ] & 0x01 ) << 3 ; unsigned char bit4 = ( in [ firstIndexToGrab + 4 ] & 0x01 ) << 4 ; unsigned char bit5 = ( in [ firstIndexToGrab + 5 ] & 0x01 ) << 5 ; unsigned char bit6 = ( in [ firstIndexToGrab + 6 ] & 0x01 ) << 6 ; unsigned char bit7 = ( in [ firstIndexToGrab + 7 ] & 0x01 ) << 7 ; unsigned char output = bit7 | bit6 | bit5 | bit4 | bit3 | bit2 | bit1 | bit0 ; int outputIndex = i * 8 + channel - 1 ; out [ outputIndex ] = output ; }
__global__ void Match ( float * P , float * Q , int q_points , int * idx ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; float min = 100000 ; float d ; float xp = P [ 0 + i * 3 ] ; float yp = P [ 1 + i * 3 ] ; float zp = P [ 2 + i * 3 ] ; float xq , yq , zq ; int j ; for ( j = 0 ; j < q_points / 2 ; j ++ ) { xq = Q [ 0 + j * 3 ] ; yq = Q [ 1 + j * 3 ] ; zq = Q [ 2 + j * 3 ] ; d = ( xp - xq ) * ( xp - xq ) + ( yp - yq ) * ( yp - yq ) + ( zp - zq ) * ( zp - zq ) ; if ( d < min ) { min = d ; idx [ i ] = j ; } } for ( j = j ; j < q_points ; j ++ ) { xq = Q [ 0 + j * 3 ] ; yq = Q [ 1 + j * 3 ] ; zq = Q [ 2 + j * 3 ] ; d = ( xp - xq ) * ( xp - xq ) + ( yp - yq ) * ( yp - yq ) + ( zp - zq ) * ( zp - zq ) ; if ( d < min ) { min = d ; idx [ i ] = j ; } } }
__global__ void col2im_gpu_kernel ( const int n , const float * data_col , const int height , const int width , const int ksize , const int pad , const int stride , const int height_col , const int width_col , float * data_im ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; for ( ; index < n ; index += blockDim . x * gridDim . x ) { float val = 0 ; int w = index % width + pad ; int h = ( index / width ) % height + pad ; int c = index / ( width * height ) ; int w_col_start = ( w < ksize ) ? 0 : ( w - ksize ) / stride + 1 ; int w_col_end = min ( w / stride + 1 , width_col ) ; int h_col_start = ( h < ksize ) ? 0 : ( h - ksize ) / stride + 1 ; int h_col_end = min ( h / stride + 1 , height_col ) ; int offset = ( c * ksize * ksize + h * ksize + w ) * height_col * width_col ; int coeff_h_col = ( 1 - stride * ksize * height_col ) * width_col ; int coeff_w_col = ( 1 - stride * height_col * width_col ) ; for ( int h_col = h_col_start ; h_col < h_col_end ; ++ h_col ) { for ( int w_col = w_col_start ; w_col < w_col_end ; ++ w_col ) { val += data_col [ offset + h_col * coeff_h_col + w_col * coeff_w_col ] ; } } data_im [ index ] += val ; } }
__global__ void yuv2rgb_kernel ( int img_size , unsigned char * gpu_img_in_y , unsigned char * gpu_img_in_u , unsigned char * gpu_img_in_v , unsigned char * gpu_img_out_r , unsigned char * gpu_img_out_g , unsigned char * gpu_img_out_b ) { int rt , gt , bt ; int rt2 , gt2 , bt2 ; int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < img_size ) { rt = ( int ) ( gpu_img_in_y [ index ] + 1.402 * ( gpu_img_in_v [ index ] - 128 ) ) ; gt = ( int ) ( gpu_img_in_y [ index ] - 0.344 * ( gpu_img_in_u [ index ] - 128 ) - 0.714 * ( gpu_img_in_v [ index ] - 128 ) ) ; bt = ( int ) gpu_img_in_y [ index ] + 1.772 * ( gpu_img_in_u [ index ] - 128 ) ; rt2 = ( rt > 255 ) ? 255 : rt ; gt2 = ( gt > 255 ) ? 255 : gt ; bt2 = ( bt > 255 ) ? 255 : bt ; gpu_img_out_r [ index ] = ( rt2 < 0 ) ? 0 : rt2 ; gpu_img_out_b [ index ] = ( bt2 < 0 ) ? 0 : bt2 ; gpu_img_out_g [ index ] = ( gt2 < 0 ) ? 0 : gt2 ; } }
__global__ void get_boxes_for_nms ( const float * boxes_before_nms , const float * offset , float * boxes_for_nms , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( boxes_before_nms [ tid * 4 + 0 ] == ( -1 ) && boxes_before_nms [ tid * 4 + 1 ] == ( -1 ) && boxes_before_nms [ tid * 4 + 2 ] == ( -1 ) && boxes_before_nms [ tid * 4 + 3 ] == ( -1 ) ) { boxes_for_nms [ tid * 4 + 0 ] = ( -1 ) ; boxes_for_nms [ tid * 4 + 1 ] = ( -1 ) ; boxes_for_nms [ tid * 4 + 2 ] = ( -1 ) ; boxes_for_nms [ tid * 4 + 3 ] = ( -1 ) ; } else { boxes_for_nms [ tid * 4 + 0 ] = boxes_before_nms [ tid * 4 + 0 ] + offset [ tid ] ; boxes_for_nms [ tid * 4 + 1 ] = boxes_before_nms [ tid * 4 + 1 ] + offset [ tid ] ; boxes_for_nms [ tid * 4 + 2 ] = boxes_before_nms [ tid * 4 + 2 ] + offset [ tid ] ; boxes_for_nms [ tid * 4 + 3 ] = boxes_before_nms [ tid * 4 + 3 ] + offset [ tid ] ; } }
__global__ void eltwise_kernel ( int size , int minw , int minh , int minc , int stride , int sample , int batch , int w1 , int h1 , int c1 , float * add , int w2 , int h2 , int c2 , float * out , int sum , int mult ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id >= size ) return ; int i = id % minw ; id /= minw ; int j = id % minh ; id /= minh ; int k = id % minc ; id /= minc ; int b = id % batch ; int out_index = i * sample + w2 * ( j * sample + h2 * ( k + c2 * b ) ) ; int add_index = i * stride + w1 * ( j * stride + h1 * ( k + c1 * b ) ) ; if ( mult == 1 ) out [ out_index ] = out [ out_index ] * add [ add_index ] ; else if ( sum == 1 ) out [ out_index ] = out [ out_index ] + add [ add_index ] ; }
__global__ void decode ( const float * anchor , const float * locData , float * predictBox , int dims , float scaleClamp , int batchSize ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } for ( int i = 0 ; i < batchSize ; i ++ ) { float anchorW = anchor [ i * dims * 4 + tid * 4 + 2 ] - anchor [ i * dims * 4 + tid * 4 ] ; float anchorH = anchor [ i * dims * 4 + tid * 4 + 3 ] - anchor [ i * dims * 4 + tid * 4 + 1 ] ; float anchorCx = anchor [ i * dims * 4 + tid * 4 ] + 0.5 * anchorW ; float anchorCy = anchor [ i * dims * 4 + tid * 4 + 1 ] + 0.5 * anchorH ; float dx = locData [ i * dims * 4 + tid * 4 ] ; float dy = locData [ i * dims * 4 + tid * 4 + 1 ] ; float dw = locData [ i * dims * 4 + tid * 4 + 2 ] ; float dh = locData [ i * dims * 4 + tid * 4 + 3 ] ; if ( dw > scaleClamp ) { dw = scaleClamp ; } if ( dh > scaleClamp ) { dh = scaleClamp ; } float preCx = dx * anchorW + anchorCx ; float preCy = dy * anchorH + anchorCy ; float preW = anchorW * 0.5 ; float preH = anchorH * 0.5 ; predictBox [ i * dims * 4 + tid * 4 ] = preCx - 0.5 * preW ; predictBox [ i * dims * 4 + tid * 4 + 1 ] = preCy - 0.5 * preH ; predictBox [ i * dims * 4 + tid * 4 + 2 ] = preCx + 0.5 * preW ; predictBox [ i * dims * 4 + tid * 4 + 3 ] = preCy + 0.5 * preH ; } }
__global__ void nlf_down_forward ( const int n , const float * filters , const int channel , const int height , const int width , const int wsize , float * top_data ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index * step ; int fbase = index / channel * wsize * step ; for ( int row = 0 ; row < height ; row ++ ) { for ( int col = 0 ; col < width ; col ++ ) { float temp = 0 ; int r = row ; int c = col ; int shift = 0 * step + row * width + col ; temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; r = row - 1 ; c = col ; shift = 1 * step + row * width + col ; if ( r >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row - 1 ; c = col - 1 ; shift = 2 * step + row * width + col ; if ( r >= 0 && c >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row - 1 ; c = col + 1 ; shift = 3 * step + row * width + col ; if ( r >= 0 && c < width ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row ; c = col - 1 ; shift = 4 * step + row * width + col ; if ( c >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; top_data [ base + row * width + col ] = temp ; } } }
__global__ void nlf_filter_left_backward ( const int n , const float * bottom_data , const float * top_data , const float * temp_diff , const int channel , const int height , const int width , const int wsize , float * filters_diff ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index / step * step * channel + index % step ; int fbase = index / step * step * wsize + index % step ; int row = index % step / width ; int col = index % step % width ; for ( int i = 0 ; i < channel ; i ++ ) { filters_diff [ fbase ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col + 1 < width ) filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * top_data [ base + 1 + i * step ] ; else filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col + 1 < width && row - 1 >= 0 ) filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * top_data [ base - width + 1 + i * step ] ; else filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col + 1 < width && row + 1 < height ) filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * top_data [ base + width + 1 + i * step ] ; else filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row + 1 < height ) filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * top_data [ base + width + i * step ] ; else filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; } }
__global__ void nlf_filter_down_backward ( const int n , const float * bottom_data , const float * top_data , const float * temp_diff , const int channel , const int height , const int width , const int wsize , float * filters_diff ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index / step * step * channel + index % step ; int fbase = index / step * step * wsize + index % step ; int row = index % step / width ; int col = index % step % width ; for ( int i = 0 ; i < channel ; i ++ ) { filters_diff [ fbase ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row - 1 >= 0 ) filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * top_data [ base - width + i * step ] ; else filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row - 1 >= 0 && col - 1 >= 0 ) filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * top_data [ base - width - 1 + i * step ] ; else filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row - 1 >= 0 && col + 1 < width ) filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * top_data [ base - width + 1 + i * step ] ; else filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col - 1 >= 0 ) filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * top_data [ base - 1 + i * step ] ; else filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; } }
__global__ void nlf_up_forward ( const int n , const float * filters , const int channel , const int height , const int width , const int wsize , float * top_data ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index * step ; int fbase = index / channel * wsize * step ; for ( int row = height - 1 ; row >= 0 ; row -- ) { for ( int col = width - 1 ; col >= 0 ; col -- ) { float temp = 0 ; int r = row ; int c = col ; int shift = 0 * step + row * width + col ; temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; r = row + 1 ; c = col ; shift = 1 * step + row * width + col ; if ( r < height ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row + 1 ; c = col - 1 ; shift = 2 * step + row * width + col ; if ( r < height && c >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row + 1 ; c = col + 1 ; shift = 3 * step + row * width + col ; if ( r < height && c < width ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row ; c = col + 1 ; shift = 4 * step + row * width + col ; if ( c < width ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; top_data [ base + row * width + col ] = temp ; } } }
