__global__ void kmeans_set_zero ( int * means ) { means [ blockIdx . x * blockDim . x + threadIdx . x ] = 0 ; }
__global__ void Mul_half ( float * src , float * dst ) { int index = threadIdx . x ; if ( index < 3 ) { dst [ index ] = src [ index ] * 0.5 ; } }
__global__ void resetIndices ( long * vec_out , const long N ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < N ) { vec_out [ idx ] = idx ; } }
__global__ void set_offset_kernel ( int stride , int size , int * output ) { for ( int i = threadIdx . x ; i < size ; i += blockDim . x ) { output [ i ] = i * stride ; } }
__global__ void setSuppressed ( int * suppressed , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } suppressed [ tid ] = 0 ; }
__global__ void allDivInplaceKernel ( double * arr , double alpha , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { arr [ i ] /= alpha ; } }
__global__ void incrementArrayOnDevice ( float * a , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) a [ idx ] = a [ idx ] + 1.f ; }
__global__ void allMulInplaceKernel ( double * arr , double alpha , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { arr [ i ] *= alpha ; } }
__global__ void Init ( const long long size , const double * in , double * out ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < size ) out [ i ] = in [ i ] ; }
__global__ void subAvg ( int * input , int count , int avg ) { int index = blockDim . x * blockIdx . x + threadIdx . x ; if ( index < count ) input [ index ] = input [ index ] - avg ; }
__global__ void allExp2InplaceKernel ( double * arr , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { arr [ i ] = arr [ i ] * 9 ; } }
__global__ void vector_add ( float * a , float * b , float * c ) { int index = threadIdx . x + blockDim . x * blockIdx . x ; c [ index ] = a [ index ] + b [ index ] ; }
__global__ void setLabels ( int * output , int dims , int clsNum ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } output [ tid ] = tid % clsNum ; }
__global__ void histogram ( int n , int * color , int * bucket ) { int i = threadIdx . x + blockDim . x * blockIdx . x ; if ( i < n ) { int c = color [ i ] ; bucket [ c ] += 1 ; } }
__global__ void sigmoid_kernel ( float * input , float * output ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; output [ tid ] = 1 / ( 1 + expf ( - input [ tid ] ) ) ; }
__global__ void kernelUpdateHead ( int * head , int * d_idxs_out , int n ) { int i = threadIdx . x + blockDim . x * blockIdx . x ; if ( i < n ) { head [ d_idxs_out [ i ] ] = 1 ; } }
__global__ void const_kernel ( int N , float ALPHA , float * X , int INCX ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) X [ i * INCX ] = ALPHA ; }
__global__ void allLog2Kernel ( const double * arr , double * buf , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { buf [ i ] = arr [ i ] / 2 ; } }
__global__ void clearArray ( unsigned char * arr , const unsigned int lenght ) { unsigned int offset = blockDim . x * blockIdx . x + threadIdx . x ; unsigned int skip = gridDim . x * blockDim . x ; while ( offset < lenght ) { arr [ offset ] = 0 ; offset += skip ; } }
__global__ void Copy_List ( const int element_numbers , const float * origin_list , float * list ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < element_numbers ) { list [ i ] = origin_list [ i ] ; } }
__global__ void add ( int n , float * x , float * y ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; for ( int i = index ; i < n ; i ++ ) { y [ i ] = x [ i ] + y [ i ] ; } }
__global__ void gpu_add ( float * c , float * a , float * b , int n ) { for ( int k = threadIdx . x ; k < n ; k += blockDim . x ) { c [ k ] = a [ k ] + b [ k ] ; } }
__global__ void subtract_matrix ( float * a , float * b , float * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] - b [ idx ] ; }
__global__ void add_matrix ( float * a , float * b , float * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] + b [ idx ] ; }
__global__ void vecAdd ( float * in1 , float * in2 , float * out , int len ) { int i = threadIdx . x + blockDim . x * blockIdx . x ; if ( i < len ) out [ i ] = in1 [ i ] + in2 [ i ] ; }
__global__ void doubleArrayScalarAddKernel ( double * d_in , double * d_out , int length , double scalar ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = d_in [ tid ] + scalar ; } }
__global__ void dadd_matrix ( double * a , double * b , double * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] + b [ idx ] ; }
__global__ void test1 ( float * input , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( input [ tid * 4 ] != 0 ) { input [ tid * 4 ] = 0 ; } }
__global__ void vecAddGPU ( double * pdbA , double * pdbB , double * pdbC ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; pdbC [ i ] = pdbA [ i ] + pdbB [ i ] ; }
__global__ void doubleArrayScalarMultiplyKernel ( double * d_in , double * d_out , int length , double scalar ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = d_in [ tid ] * scalar ; } }
__global__ void addV ( int * a , int * b , int * c , int N ) { int index = threadIdx . x + blockIdx . x * blockDim . x ; if ( index < N ) { c [ index ] = a [ index ] + b [ index ] ; } }
__global__ void VecAdd ( float * A , float * B , float * C , int N ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < N ) { C [ i ] = A [ i ] + B [ i ] ; } }
__global__ void saxpy_gpu_kernel ( float * x , float * y , float alpha , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { y [ i ] = alpha * x [ i ] + y [ i ] ; } }
__global__ void sumArrays ( float * A , float * B , float * C , const int N ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < N ) C [ i ] = A [ i ] + B [ i ] ; }
__global__ void doubleArrayScalarSubtractKernel ( double * d_in , double * d_out , int length , double scalar ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = d_in [ tid ] - scalar ; } }
__global__ void doubleArrayElementwiseSquareKernel ( double * d_in , double * d_out , int length ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = pow ( d_in [ tid ] , 2 ) ; } }
__global__ void sum_array_overlap ( int * a , int * b , int * c , int N ) { int gid = blockIdx . x * blockDim . x + threadIdx . x ; if ( gid < N ) { c [ gid ] = a [ gid ] + b [ gid ] ; } }
__global__ void k_vec_divide ( float * vec1 , float * vec2 , size_t max_size ) { for ( int i = blockIdx . x * blockDim . x + threadIdx . x ; i < max_size ; i += blockDim . x * gridDim . x ) { vec1 [ i ] = vec1 [ i ] / vec2 [ i ] ; } }
__global__ void saxpi_nBlock ( int n , float a , float * x , float * y ) { int idx = threadIdx . x + ( blockIdx . x * blockDim . x ) ; if ( idx < n ) { y [ idx ] = a * x [ idx ] + y [ idx ] ; } }
__global__ void cuda_record ( float * p , float * seis_kt , int * Gxz , int ng ) { int id = threadIdx . x + blockDim . x * blockIdx . x ; if ( id < ng ) seis_kt [ id ] = p [ Gxz [ id ] ] ; }
__global__ void vectorDiv ( const float * A , const float * B , float * C , int numElements ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < numElements ) { C [ i ] = A [ i ] / B [ i ] ; } }
__global__ void kernelSAXPY ( int len , float a , float * d_x , float * d_y ) { const int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < len ) d_y [ i ] = d_x [ i ] * a + d_y [ i ] ; }
__global__ void vectorAdd ( const float * A , const float * B , float * C , int numElements ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < numElements ) { C [ i ] = A [ i ] + B [ i ] ; } }
__global__ void vectorAdd ( double * a , double * b , double * c , int vector_size ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < vector_size ) { c [ tid ] = a [ tid ] + b [ tid ] ; } }
__global__ void intAdd ( int * c , const int * a , const int * b , const unsigned int d ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; if ( i < d ) { c [ i ] = a [ i ] + b [ i ] ; } }
__global__ void histo_atomic ( const unsigned int * const vals , unsigned int * const histo , int numVals ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; if ( i >= numVals ) return ; atomicAdd ( & histo [ vals [ i ] ] , 1 ) ; }
__global__ void vadd ( const float * a , const float * b , float * c , const unsigned int count ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < count ) { c [ i ] = a [ i ] + b [ i ] ; } }
__global__ void intSubtract ( int * c , const int * a , const int * b , const unsigned int d ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; if ( i < d ) { c [ i ] = a [ i ] + b [ i ] ; } }
__global__ void transferMBR3 ( double * xy_copy , long long * a_copy , int tasks ) { for ( int i = blockIdx . x * blockDim . x + threadIdx . x ; i < tasks ; i += blockDim . x * gridDim . x ) { a_copy [ i ] = xy_copy [ i ] * 10000000 ; } }
__global__ void binarize_kernel ( float * x , int n , float * binary ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i >= n ) return ; binary [ i ] = ( x [ i ] >= 0 ) ? 1 : -1 ; }
__global__ void add_vec_scalaire_gpu ( int * vec , int * res , int a , long N ) { long i = ( long ) blockIdx . x * ( long ) blockDim . x + ( long ) threadIdx . x ; if ( i < N ) { res [ i ] = vec [ i ] + a ; } }
__global__ void memcpy_kernel ( int * dst , int * src , size_t n ) { int num = gridDim . x * blockDim . x ; int id = blockDim . x * blockIdx . x + threadIdx . x ; for ( int i = id ; i < n / sizeof ( int ) ; i += num ) { dst [ i ] = src [ i ] ; } }
__global__ void add_kernel ( int N , float ALPHA , float * X , int INCX ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) X [ i * INCX ] += ALPHA ; }
__global__ void doubleArraySignKernel ( double * d_in , double * d_out , int length ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = ( 0 < d_in [ tid ] ) - ( d_in [ tid ] < 0 ) ; } }
__global__ void find_max_among_blocks ( int * data , int blockSize , int nbBlocks ) { for ( int i = 0 ; i < nbBlocks ; ++ i ) { if ( data [ 0 ] < data [ i * blockSize ] ) data [ 0 ] = data [ i * blockSize ] ; } }
__global__ void setOffset ( int * offset , int dims , int batchSize ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid > 0 ) { return ; } offset [ 0 ] = 0 ; for ( int i = 1 ; i < batchSize + 1 ; i ++ ) { offset [ i ] = i * dims ; } }
__global__ void expandScoreFactors ( const float * input , float * output , int dims , int clsNum ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } int k = tid / clsNum ; output [ tid ] = input [ k ] ; }
__global__ void kernelIsFirst ( int * head , int * first_pts , int n ) { int i = threadIdx . x + blockDim . x * blockIdx . x ; if ( i < n ) { if ( head [ i ] == 1 ) first_pts [ i ] = i ; else first_pts [ i ] = 0 ; } }
__global__ void sumRowKernel ( int * d_in , int * d_out , int DIM ) { for ( int bid = blockIdx . x ; bid < DIM ; bid += gridDim . x ) { int sum = 0 ; for ( int tid = threadIdx . x ; tid < DIM ; tid += blockDim . x ) { sum += d_in [ tid + bid * DIM ] ; } atomicAdd ( & d_out [ bid ] , sum ) ; } }
__global__ void addVectorsInto ( float * result , float * a , float * b , int N ) { int index = threadIdx . x + blockIdx . x * blockDim . x ; int stride = blockDim . x * gridDim . x ; for ( int i = index ; i < N ; i += stride ) { result [ i ] = a [ i ] + b [ i ] ; } }
__global__ void setIndexYolov3 ( int * input , int dims , int batchSize ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } for ( int i = 0 ; i < batchSize ; i ++ ) { input [ i * dims + tid ] = tid ; } }
__global__ void shiftIndices ( long * vec_out , const long by , const long imageSize , const long N ) { long idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < N ) { vec_out [ idx ] = ( imageSize + ( ( idx - N / 2 + by ) % imageSize ) ) % imageSize ; } }
__global__ void doubleArrayVectorSubtractKernel ( double * d_in_a , double * d_in_b , double * d_out , int length ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = d_in_a [ tid ] - d_in_b [ tid ] ; } }
__global__ void doubleArrayVectorElementwiseMultiplyKernel ( double * d_in_a , double * d_in_b , double * d_out , int length ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = d_in_a [ tid ] * d_in_b [ tid ] ; } }
__global__ void fill_idx ( int N , int * device_input , int * device_output ) { int idx = blockDim . x * blockIdx . x + threadIdx . x ; if ( idx + 1 < N && device_input [ idx ] + 1 == device_input [ idx + 1 ] ) { device_output [ device_input [ idx ] ] = idx ; } }
__global__ void gpuSearchPosShmem1EQ ( int key , int * devKey , int * devPos , int size ) { int globalTx = blockIdx . x * blockDim . x + threadIdx . x ; if ( globalTx < size ) { if ( devKey [ globalTx ] == key ) { devPos [ 0 ] = globalTx ; } } }
__global__ void mathKernel1 ( float * c ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; float ia , ib ; ia = ib = 0.0f ; if ( tid % 2 == 0 ) { ia = 100.0f ; } else { ib = 200.0f ; } c [ tid ] = ia + ib ; }
__global__ void reverseArrayBlock ( int * d_out , int * d_in ) { extern __shared__ int s_data [ ] ; int i = blockIdx . x * blockDim . x + threadIdx . x ; int j = ( gridDim . x - 1 - blockIdx . x ) * blockDim . x + threadIdx . x ; s_data [ blockDim . x - 1 - threadIdx . x ] = d_in [ i ] ; __syncthreads ( ) ; d_out [ j ] = s_data [ threadIdx . x ] ; }
__global__ void gaussianPass ( int patchSize , int dataSize , float * gaussFilter , float * data ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = blockDim . x * gridDim . x ; for ( int i = index ; i < dataSize ; i += stride ) { data [ i ] = gaussFilter [ i % ( patchSize * patchSize ) ] * data [ i ] ; } }
__global__ void addKernel ( int * a , int * b , int * c , int vectorSize , int elements_per_thread ) { int start = ( blockIdx . x * blockDim . x + threadIdx . x ) * elements_per_thread ; for ( int i = start ; i - start < elements_per_thread && ( i < vectorSize ) ; i ++ ) { c [ i ] = a [ i ] + b [ i ] ; } }
__global__ void clamp_kernel ( int N , float * X , int INCX , float clamp_min , float clamp_max ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) X [ i * INCX ] = fminf ( clamp_max , fmaxf ( clamp_min , X [ i * INCX ] ) ) ; }
__global__ void histogram ( int * x , int * bins , int n ) { auto i = threadIdx . x + blockIdx . x * blockDim . x ; if ( i < n ) { const auto c = x [ i ] ; atomicAdd ( & bins [ c ] , 1 ) ; } }
__global__ void multMat ( int n , int * arrForce_d , int * arrDistance_d , int * arrAnswer_d ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { arrAnswer_d [ i ] = arrForce_d [ i ] * arrDistance_d [ i ] ; } }
__global__ void axpy_kernel ( int N , float ALPHA , float * X , int OFFX , int INCX , float * Y , int OFFY , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) Y [ OFFY + i * INCY ] += ALPHA * X [ OFFX + i * INCX ] ; }
__global__ void castImageTofloat ( float * deviceOutputImageData , unsigned char * ucharImage , int imageWidth , int imageHeight , int channels , int pixelSize ) { int w = threadIdx . x + blockDim . x * blockIdx . x ; if ( w < pixelSize ) deviceOutputImageData [ w ] = ( float ) ( ucharImage [ w ] / 255.0 ) ; }
__global__ void zero_centroid_vals ( int k , double * __restrict__ Cx_sum , double * __restrict__ Cy_sum , int * __restrict__ Csize ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < k ) { Cx_sum [ index ] = 0 ; Cy_sum [ index ] = 0 ; Csize [ index ] = 0 ; } }
__global__ void HammingDistance ( int * c , const int * a , const int * b , long const int * size ) { int i = threadIdx . x + blockDim . x * blockIdx . x ; int stride = blockDim . x * gridDim . x ; for ( ; i < * size ; i += stride ) { a [ i ] != b [ i ] ? atomicAdd ( c , 1 ) : 1 ; } }
__global__ void castImageToUchar ( float * deviceInputImageData , unsigned char * ucharImage , int imageWidth , int imageHeight , int channels , int pixelSize ) { int w = threadIdx . x + blockDim . x * blockIdx . x ; if ( w < pixelSize ) ucharImage [ w ] = ( unsigned char ) ( 255 * deviceInputImageData [ w ] ) ; }
__global__ void compareDoubleArrayToThresholdKernel ( double * d_in , int * d_out , int length , double threshold ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; double abs = d_in [ tid ] > 0 ? d_in [ tid ] : - d_in [ tid ] ; if ( tid < length ) { d_out [ tid ] = ( abs < threshold ) ; } }
__global__ void transpose ( float * a , float * b , int width ) { float result = 0 ; int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; for ( int k = 0 ; k < width ; k ++ ) { b [ k * width + col ] = a [ row * width + k ] ; } }
__global__ void cuda_ReLU_forward_kernel ( float * d_in_data , bool * d_mask , const long unsigned int datasize , bool training ) { uint i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= datasize ) return ; bool keep = d_in_data [ i ] > 0 ; if ( training ) d_mask [ i ] = keep ; if ( ! keep ) d_in_data [ i ] = 0 ; }
__global__ void kernel_sum_backward ( float * db , float * dout , int r , int c ) { unsigned int tid = blockDim . x * blockIdx . x + threadIdx . x ; int N = c ; while ( tid < N ) { for ( int i = 0 ; i < r ; i ++ ) { db [ tid ] += dout [ i * c + tid ] ; } tid += gridDim . x * blockDim . x ; } }
__global__ void clip_kernel ( int N , float ALPHA , float * X , int INCX , float * Y , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) { float val = X [ i * INCX ] ; Y [ i * INCY ] = val > ALPHA ? val : 0 ; } }
__global__ void update_x ( double * x , double * a , double * b , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = gridDim . x * blockDim . x ; for ( int i = index ; i < n ; i += stride ) { x [ i ] = 2. / 3. * a [ i ] / b [ i ] + 1. / 3. * x [ i ] ; } }
__global__ void naiveParTrans ( float * d_in , float * d_out , int x_size , int y_size ) { int gidx = blockIdx . x * blockDim . x + threadIdx . x ; int gidy = blockIdx . y * blockDim . y + threadIdx . y ; if ( gidx < x_size && gidy < y_size ) { d_out [ gidx * y_size + gidy ] = d_in [ gidy * x_size + gidx ] ; } }
__global__ void l2_kernel ( int n , float * pred , float * truth , float * delta , float * error ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { float diff = truth [ i ] - pred [ i ] ; error [ i ] = diff * diff ; delta [ i ] = diff ; } }
__global__ void flipKernel ( float * array1 , int width ) { int current_index = blockIdx . x * blockDim . x + threadIdx . x ; int replace = ( width - 1 - current_index / width ) * width + current_index % width ; if ( current_index < width * width / 2 ) { float temp = array1 [ current_index ] ; array1 [ current_index ] = array1 [ replace ] ; array1 [ replace ] = temp ; } }
__global__ void get_conf_inds ( const float * mlvl_conf , const float conf_thr , int * conf_inds , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( mlvl_conf [ tid ] >= conf_thr ) { conf_inds [ tid ] = 1 ; } else { conf_inds [ tid ] = -1 ; } }
__global__ void gpuSearchPosShmem1 ( int key , int * gpu_key_arr , int * gpu_pos , int size ) { int globalTx = blockIdx . x * blockDim . x + threadIdx . x ; if ( globalTx < size ) { if ( key >= gpu_key_arr [ globalTx ] && key < gpu_key_arr [ globalTx + 1 ] ) { * gpu_pos = globalTx ; } } }
__global__ void HistogramKernel ( int * hist , unsigned char * img_in , int img_size ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= img_size ) return ; int stride = blockDim . x * gridDim . x ; while ( i < img_size ) { atomicAdd ( & ( hist [ img_in [ i ] ] ) , 1 ) ; i += stride ; } __syncthreads ( ) ; }
__global__ void mean ( float * A , float * means , int size_row , int size_col ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < size_col ) { for ( int i = 0 ; i < size_row ; i ++ ) { means [ idx ] += A [ idx * size_row + i ] ; } means [ idx ] = means [ idx ] / size_row ; } }
__device__ float sumreduce ( float in ) { extern __shared__ float sdata [ ] ; unsigned int tid = threadIdx . x ; sdata [ tid ] = in ; __syncthreads ( ) ; for ( unsigned int s = blockDim . x / 2 ; s > 0 ; s >>= 1 ) { if ( tid < s ) { sdata [ tid ] += sdata [ tid + s ] ; } __syncthreads ( ) ; } return sdata [ 0 ] ; }
__global__ void pythagoras ( unsigned char * a , unsigned char * b , unsigned char * c ) { int idx = ( blockIdx . x * blockDim . x ) + threadIdx . x ; float af = float ( a [ idx ] ) ; float bf = float ( b [ idx ] ) ; c [ idx ] = ( unsigned char ) sqrtf ( af * af + bf * bf ) ; }
__global__ void avgpool ( int n , float * input_im , float * output_im ) { int class_index = blockIdx . x * blockDim . x + threadIdx . x ; if ( class_index < n ) { input_im += 169 * class_index ; float tmp = 0.0f ; for ( int i = 0 ; i < 169 ; i ++ ) { tmp += input_im [ i ] ; } output_im [ class_index ] = tmp / 169.0 ; } }
__global__ void weighted_sum_kernel ( int n , float * a , float * b , float * s , float * c ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { c [ i ] = s [ i ] * a [ i ] + ( 1 - s [ i ] ) * ( b ? b [ i ] : 0 ) ; } }
__global__ void kernel ( float * x , int n ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; for ( int i = tid ; i < n ; i += blockDim . x * gridDim . x ) { double sum = 0 ; for ( int j = 0 ; j < 1000 ; j ++ ) { sum += sqrt ( pow ( 3.14159 , i ) ) / float ( j ) ; } x [ i ] = sum ; } }
__global__ void MMDSelfComputeWithSum ( float * x_average , int size_x , float * distance_matrix ) { int block_id = blockIdx . x ; int thread_id = threadIdx . x ; for ( int i = block_id ; i < size_x ; i += gridDim . x ) { for ( int j = thread_id + i ; j < size_x ; j += blockDim . x ) { distance_matrix [ i * size_x + j ] = x_average [ i ] * x_average [ j ] ; } } }
__global__ void matVecRowSubKernel ( const double * mat , const double * vec , double * buf , int m , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < m * n ) { int i = index / n ; int j = index % n ; buf [ i * n + j ] = mat [ i * n + j ] - vec [ j ] ; } }
__global__ void transKernel ( float * array1 , float * array2 , int width ) { int current_index = ( blockIdx . y * blockDim . y + threadIdx . y ) * width + ( blockIdx . x * blockDim . x + threadIdx . x ) ; int replace = ( blockIdx . x * blockDim . x + threadIdx . x ) * width + blockIdx . y * blockDim . y + threadIdx . y ; if ( current_index < width * width ) { array2 [ replace ] = array1 [ current_index ] ; } }
__global__ void rowSumSquareKernel ( const double * mat , double * buf , int m , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < m ) { double sum = 0.0 ; for ( int j = 0 ; j < n ; j ++ ) { double a = mat [ i * n + j ] ; sum += a * a ; } buf [ i ] = sum ; } }
__global__ void matrixTranspose ( int * in_mat , int * out_mat , int dim_rows , int dim_cols ) { int row = threadIdx . y + blockIdx . y * blockDim . y ; int col = threadIdx . x + blockIdx . x * blockDim . x ; if ( row < dim_rows && col < dim_cols ) { unsigned int new_pos = col * dim_cols + row ; out_mat [ new_pos ] = in_mat [ row * dim_cols + col ] ; } }
__global__ void analysis ( int D [ ] , int L [ ] , int R [ ] , int N ) { int id = blockIdx . x * blockDim . x + blockIdx . y * blockDim . x * gridDim . x + threadIdx . x ; if ( id >= N ) return ; int label = L [ id ] ; int ref ; if ( label == id ) { do { label = R [ ref = label ] ; } while ( ref ^ label ) ; R [ id ] = label ; } }
__global__ void Kernel_Transpose2d ( float * dev_transposeArray , float * dev_array , const int r , const int c ) { unsigned int i = blockDim . x * blockIdx . x + threadIdx . x ; unsigned int j = blockDim . y * blockIdx . y + threadIdx . y ; if ( i >= r || j >= c ) return ; int idx_transposeArray , idx_array ; idx_array = i * c + j ; idx_transposeArray = j * r + i ; dev_transposeArray [ idx_transposeArray ] = dev_array [ idx_array ] ; }
__global__ void getMeanImage ( const double * images , double * meanImage , int imageNum , int pixelNum ) { int col = blockIdx . x * blockDim . x + threadIdx . x ; if ( col >= pixelNum ) { return ; } meanImage [ col ] = 0.0 ; for ( int row = 0 ; row < imageNum ; ++ row ) { meanImage [ col ] += images [ row * pixelNum + col ] ; } meanImage [ col ] /= imageNum ; }
__global__ void Kernel_Avg ( float * dev_arrayMax , float * dev_array , const int r , const int c ) { unsigned int tid = blockDim . x * blockIdx . x + threadIdx . x ; int N = r ; float sum ; int i ; while ( tid < N ) { i = tid ; sum = 0.0 ; for ( int j = 0 ; j < c ; j ++ ) { sum += dev_array [ i * c + j ] ; } dev_arrayMax [ i ] = sum / c ; tid += gridDim . x * blockDim . x ; } }
__global__ void smallCorrelation ( float * L , float * innerSums , int innerSumsLength ) { int u = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( u >= innerSumsLength ) return ; int realIdx = 2 * u ; int imagIdx = realIdx + 1 ; L [ u ] = ( innerSums [ realIdx ] * innerSums [ realIdx ] ) + ( innerSums [ imagIdx ] * innerSums [ imagIdx ] ) ; }
__global__ void cudaDecodeBitstream ( unsigned short * encoded , unsigned short * decoded , int size ) { int bit_index = ( ( ( blockIdx . x * blockDim . x ) + threadIdx . x ) * 2 ) + 2 ; if ( bit_index >= size ) return ; unsigned short curr_bit = encoded [ bit_index ] ; decoded [ bit_index ] = ! encoded [ bit_index - 1 ] ^ curr_bit ; decoded [ bit_index + 1 ] = curr_bit ^ encoded [ bit_index + 1 ] ; }
__global__ void vectorMatrixMult ( long int totalPixels , float * matrix , float * vector , float * out ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = blockDim . x * gridDim . x ; for ( long int i = index ; i < totalPixels ; i += stride ) { float sum = 0.0 ; for ( long int j = 0 ; j < totalPixels ; j ++ ) { sum += matrix [ i * totalPixels + j ] * vector [ j ] ; } out [ i ] = sum ; } }
__global__ void roundOff ( float * mat , int N ) { int i = threadIdx . x ; int j = blockIdx . x ; if ( mat [ i * N + j ] >= 0 ) mat [ i * N + j ] = ( int ) ( mat [ i * N + j ] + 0.5 ) ; else mat [ i * N + j ] = ( int ) ( mat [ i * N + j ] - 0.5 ) ; }
__global__ void update_clusters ( int n , int k , double * Cx , double * Cy , double * Cx_sum , double * Cy_sum , int * Csize ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < k && Csize [ index ] ) { Cx [ index ] = Cx_sum [ index ] / Csize [ index ] ; Cy [ index ] = Cy_sum [ index ] / Csize [ index ] ; } }
__global__ void GatherKernel ( const int * input , float * output , int input_size , const float * data , int count , int dim , int data_offset ) { const int thread_index = blockIdx . x * blockDim . x + threadIdx . x ; if ( thread_index < input_size * dim ) { const int input_id = input [ thread_index / dim ] ; const int pos = thread_index % dim ; if ( input_id < count + data_offset && input_id >= data_offset ) { output [ thread_index ] = data [ input_id * dim + pos ] ; } } }
__global__ void histogrammPrimitive ( unsigned int * histogrammVector , unsigned char * grayImage , int rows , int columns ) { int column = blockIdx . x * blockDim . x + threadIdx . x ; int row = blockIdx . y * blockDim . y + threadIdx . y ; int offset = ( column ) + ( columns * row ) ; if ( ( column < columns ) && ( row < rows ) ) { unsigned char grayValue = grayImage [ offset ] ; atomicAdd ( & ( histogrammVector [ grayValue ] ) , 1 ) ; } }
__global__ void sumAndScale ( float * noiseVariance , float * diffMag2 , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= n ) return ; int batchJump = i * 347 ; float temp ; temp = 0 ; for ( int sumIndex = 0 ; sumIndex < 347 ; sumIndex ++ ) temp += diffMag2 [ batchJump + sumIndex ] ; temp = .00161812 * temp ; noiseVariance [ i ] = temp ; }
__global__ void convertInstanceToLabel_Kernel ( unsigned short * d_outputLabel , const unsigned char * d_inputInstance , const unsigned short * d_instanceToLabel , unsigned int width , unsigned int height ) { const unsigned int x = blockIdx . x * blockDim . x + threadIdx . x ; const unsigned int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( x < width && y < height ) { d_outputLabel [ y * width + x ] = d_instanceToLabel [ d_inputInstance [ y * width + x ] ] ; } }
__global__ void downsampleCuda ( float * I , float * Q , unsigned int numDownsampledSamples , float * downsampled_I , float * downsampled_Q , unsigned int factor ) { int sampleIndex = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( sampleIndex >= numDownsampledSamples ) return ; unsigned int absoluteIndex = sampleIndex * factor ; downsampled_I [ sampleIndex ] = I [ absoluteIndex ] ; downsampled_Q [ sampleIndex ] = Q [ absoluteIndex ] ; }
__global__ void logistic_x_ent_kernel ( int n , float * pred , float * truth , float * delta , float * error ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { float t = truth [ i ] ; float p = pred [ i ] ; error [ i ] = - t * log ( p + .0000001 ) - ( 1 - t ) * log ( 1 - p + .0000001 ) ; delta [ i ] = t - p ; } }
__global__ void Kernel_Argmax ( int * dev_argMax , float * dev_array , const int r , const int c ) { unsigned int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i >= r ) return ; int idx ; float temp = 0.0 ; for ( int j = 0 ; j < c ; j ++ ) { if ( dev_array [ i * c + j ] > temp ) { temp = dev_array [ i * c + j ] ; idx = j ; } } dev_argMax [ i ] = idx ; }
__global__ void kernel_CEE ( float * x , int * t , float * loss , int r , int c ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; int N = r ; float temp ; while ( i < N ) { for ( int j = 0 ; j < c ; j ++ ) { if ( t [ i * c + j ] == 1 ) { temp = logf ( x [ i * c + j ] + 1e-7 ) ; atomicAdd ( loss , temp ) ; continue ; } } i += gridDim . x * blockDim . x ; } }
__global__ void check_results_kernel ( uint * g_results0 , uint * g_results1 , int n ) { uint idx = threadIdx . x ; uint gidx = blockDim . x * blockIdx . x + idx ; uint result0 ; uint result1 ; if ( gidx < n ) { result0 = g_results0 [ gidx ] ; result1 = g_results1 [ gidx ] ; if ( result0 != result1 ) { printf ( " % i ▁ ! = ▁ % i ▁ for ▁ thread ▁ % i ▁ \n " , result0 , result1 , gidx ) ; } } }
__global__ void MatrixMulKernel ( float * Md , float * Nd , float * Pd , int width ) { int tx = threadIdx . x ; int ty = threadIdx . y ; float pvalue = 0 ; for ( int k = 0 ; k < width ; ++ k ) { float Mdelement = Md [ ty * width + k ] ; float Ndelement = Nd [ ty * width + k ] ; pvalue += Mdelement * Ndelement ; } Pd [ ty * width + tx ] = pvalue ; }
__global__ void normalize_kernel ( int N , float * x , float * mean , float * variance , int batch , int filters , int spatial ) { int index = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( index >= N ) return ; int f = ( index / spatial ) % filters ; x [ index ] = ( x [ index ] - mean [ f ] ) / ( sqrtf ( variance [ f ] + .00001f ) ) ; }
__global__ void kernel ( int * a , int * b , int * c ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; if ( idx < ( 1024 * 1024 ) ) { int idx1 = ( idx + 1 ) % 256 ; int idx2 = ( idx + 2 ) % 256 ; float as = ( a [ idx ] + a [ idx1 ] + a [ idx2 ] ) / 3.0f ; float bs = ( b [ idx ] + b [ idx1 ] + b [ idx2 ] ) / 3.0f ; c [ idx ] = ( as + bs ) / 2 ; } }
__global__ void kernel ( int * a , int * b , int * c ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; if ( idx < 1024 * 1024 ) { int idx1 = ( idx + 1 ) % 256 ; int idx2 = ( idx + 2 ) % 256 ; float as = ( a [ idx ] + a [ idx1 ] + a [ idx2 ] ) / 3.0f ; float bs = ( b [ idx ] + b [ idx1 ] + b [ idx2 ] ) / 3.0f ; c [ idx ] = ( as + bs ) / 2 ; } }
__global__ void inter_kernel ( int NX , float * X , int NY , float * Y , int B , float * OUT ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < ( NX + NY ) * B ) { int b = i / ( NX + NY ) ; int j = i % ( NX + NY ) ; if ( j < NX ) { OUT [ i ] = X [ b * NX + j ] ; } else { OUT [ i ] = Y [ b * NY + j - NX ] ; } } }
__global__ void manage_adj_matrix ( float * gpu_graph , int n ) { int id = blockIdx . x * blockDim . x + threadIdx . x ; if ( id < n ) { float sum = 0.0 ; for ( int i = 0 ; i < n ; ++ i ) { sum += gpu_graph [ i * n + id ] ; } for ( int i = 0 ; i < n ; ++ i ) { if ( sum != 0.0 ) { gpu_graph [ i * n + id ] /= sum ; } else { gpu_graph [ i * n + id ] = ( 1 / ( float ) n ) ; } } } }
__global__ void faKernel ( const float * __restrict__ q , const float * __restrict__ h , int nq , float * __restrict__ a , float * __restrict__ fa ) { int iq = blockIdx . x * blockDim . x + threadIdx . x ; if ( iq < ( nq - 1 ) ) { float dq = q [ 1 ] - q [ 0 ] ; a [ iq ] = ( h [ iq + 1 ] * q [ iq + 1 ] - h [ iq ] * q [ iq ] ) / dq ; fa [ iq ] = q [ iq ] * ( a [ iq ] - h [ iq ] ) + 1.0f ; } }
__global__ void matrixProduct ( double * matrix_a , double * matrix_b , double * matrix_c , int width , int from , int my_rank ) { int row = threadIdx . y + blockDim . y * blockIdx . y ; int col = threadIdx . x + blockDim . x * blockIdx . x ; matrix_c [ row * width + col ] = 0 ; for ( int k = 0 ; k < width ; k ++ ) { matrix_c [ row * width + col ] += matrix_a [ ( ( row + from ) * width ) + k ] * matrix_b [ k * width + col ] ; } }
__global__ void expandBoxes ( const float * input , float * output , int dims , int clsNum ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } int k = tid / clsNum ; output [ tid * 4 + 0 ] = input [ k * 4 + 0 ] ; output [ tid * 4 + 1 ] = input [ k * 4 + 1 ] ; output [ tid * 4 + 2 ] = input [ k * 4 + 2 ] ; output [ tid * 4 + 3 ] = input [ k * 4 + 3 ] ; }
__global__ void equalization ( float * cdf , float * mincdf , unsigned char * ucharImage , int imageWidth , int imageHeight , int channels , int pixelSize ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < pixelSize ) { unsigned char val = ucharImage [ idx ] ; float data = 255 * ( cdf [ val ] - mincdf [ 0 ] ) / ( 1 - mincdf [ 0 ] ) ; if ( data < 0.0f ) data = 0.0f ; else if ( data > 255.0f ) data = 255.0f ; ucharImage [ idx ] = ( unsigned char ) data ; } }
__global__ void smooth_l1_kernel ( int n , float * pred , float * truth , float * delta , float * error ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { float diff = truth [ i ] - pred [ i ] ; float abs_val = fabsf ( diff ) ; if ( abs_val < 1 ) { error [ i ] = diff * diff ; delta [ i ] = diff ; } else { error [ i ] = 2 * abs_val - 1 ; delta [ i ] = ( diff > 0 ) ? 1 : -1 ; } } }
__global__ void matrixMult ( float * A , float * B , float * C , int width ) { int k = 0 ; float sum = 0 ; int col = blockDim . x * blockIdx . x + threadIdx . x ; int row = blockDim . y * blockIdx . y + threadIdx . y ; if ( col < width && row < width ) { for ( k = 0 ; k < width ; k ++ ) sum += A [ row * width + k ] * B [ k * width + col ] ; C [ row * width + col ] = sum ; } }
__global__ void SetToZero_kernel ( float * d_vx , float * d_vy , float * d_vz , int w , int h , int l ) { unsigned int i = blockIdx . x * blockDim . x + threadIdx . x ; unsigned int j = blockIdx . y * blockDim . y + threadIdx . y ; unsigned int index = j * w + i ; if ( i < w && j < h ) { for ( int k = 0 ; k < l ; ++ k , index += w * h ) { d_vx [ index ] = 0 ; d_vy [ index ] = 0 ; d_vz [ index ] = 0 ; } } }
__global__ void solveLowerKernel ( const double * lower , const double * b , double * buf , int dim , int n ) { int k = blockIdx . x * blockDim . x + threadIdx . x ; if ( k < n ) { for ( int i = 0 ; i < dim ; i ++ ) { double val = b [ k * dim + i ] ; for ( int j = 0 ; j < i ; j ++ ) { val -= lower [ i * dim + j ] * buf [ k * dim + j ] ; } buf [ k * dim + i ] = val / lower [ i * dim + i ] ; } } }
__global__ void deinter_kernel ( int NX , float * X , int NY , float * Y , int B , float * OUT ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < ( NX + NY ) * B ) { int b = i / ( NX + NY ) ; int j = i % ( NX + NY ) ; if ( j < NX ) { if ( X ) X [ b * NX + j ] += OUT [ i ] ; } else { if ( Y ) Y [ b * NY + j - NX ] += OUT [ i ] ; } } }
__global__ void binarize_input_kernel ( float * input , int n , int size , float * binary ) { int s = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( s >= size ) return ; int i = 0 ; float mean = 0 ; for ( i = 0 ; i < n ; ++ i ) { mean += abs ( input [ i * size + s ] ) ; } mean = mean / n ; for ( i = 0 ; i < n ; ++ i ) { binary [ i * size + s ] = ( input [ i * size + s ] > 0 ) ? mean : - mean ; } }
__global__ void matrixMultiply ( float * A , float * B , float * C , int numARows , int numAColumns , int numBRows , int numBColumns ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; int numCRows = numARows ; int numCColumns = numBColumns ; if ( row < numCRows && col < numCColumns ) { float sum = 0 ; for ( int k = 0 ; k < numBRows ; k ++ ) { sum += A [ row * numAColumns + k ] * B [ k * numBColumns + col ] ; } C [ row * numCColumns + col ] = sum ; } }
__global__ void backward_avgpool_layer_kernel ( int n , int w , int h , int c , float * in_delta , float * out_delta ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id >= n ) return ; int k = id % c ; id /= c ; int b = id ; int i ; int out_index = ( k + c * b ) ; for ( i = 0 ; i < w * h ; ++ i ) { int in_index = i + h * w * ( k + b * c ) ; in_delta [ in_index ] += out_delta [ out_index ] / ( w * h ) ; } }
__global__ void mean_kernel ( float * x , int batch , int filters , int spatial , float * mean ) { float scale = 1.f / ( batch * spatial ) ; int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i >= filters ) return ; int j , k ; mean [ i ] = 0 ; for ( j = 0 ; j < batch ; ++ j ) { for ( k = 0 ; k < spatial ; ++ k ) { int index = j * filters * spatial + i * spatial + k ; mean [ i ] += x [ index ] ; } } mean [ i ] *= scale ; }
__global__ void boxesScale ( const float * input , float * output , int dims , float scale0 , float scale1 , float scale2 , float scale3 ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } output [ tid * 4 ] = input [ tid * 4 ] / scale0 ; output [ tid * 4 + 1 ] = input [ tid * 4 + 1 ] / scale1 ; output [ tid * 4 + 2 ] = input [ tid * 4 + 2 ] / scale2 ; output [ tid * 4 + 3 ] = input [ tid * 4 + 3 ] / scale3 ; }
__global__ void mat_mul_kernel ( int * m_A , int * m_B , int * m_C , int A_rows , int A_cols , int B_rows , int B_cols ) { int sum = 0 ; int row = threadIdx . y + blockIdx . y * blockDim . y ; int col = threadIdx . x + blockIdx . x * blockDim . x ; if ( row < A_rows && col < B_cols ) { for ( int i = 0 ; i < A_cols ; i ++ ) { sum += m_A [ row * A_cols + i ] * m_B [ i * B_cols + col ] ; } m_C [ row * B_cols + col ] = sum ; } }
__global__ void create_p_vect ( float * node_info1 , float * node_info2 , float * p , int n_nodes_1 , int n_nodes_2 ) { int tx = threadIdx . x + blockDim . x * blockIdx . x ; int ty = threadIdx . y + blockDim . y * blockIdx . y ; float cutoff = 0.5 ; if ( ( tx < n_nodes_1 ) && ( ty < n_nodes_2 ) ) { int ind = tx * n_nodes_2 + ty ; if ( ( node_info1 [ tx ] < cutoff ) && ( node_info2 [ ty ] < cutoff ) ) p [ ind ] = 0 ; else p [ ind ] = node_info1 [ tx ] * node_info2 [ ty ] ; } }
__global__ void calcbidvalues ( int n , int * src2tgt , float * adj , float * prices , bool * complete , float * values , float * bids ) { int INDEX = blockIdx . x * blockDim . x + threadIdx . x ; int stride = blockDim . x * gridDim . x ; for ( int idx = INDEX ; idx < n * n ; idx += stride ) { int i = idx / n ; int j = idx - i * n ; bids [ i * n + j ] = -1 ; if ( src2tgt [ i ] != -1 ) { continue ; } complete [ 0 ] = false ; values [ i * n + j ] = - adj [ i * n + j ] - prices [ j ] ; } }
__global__ void kernel_rows ( const float * filter , const float * input , float * output , int imageW , int imageH , int filterR ) { int idx_x = threadIdx . x + blockDim . x * blockIdx . x ; int idx_y = threadIdx . y + blockDim . y * blockIdx . y ; int grid_width = gridDim . x * blockDim . x ; int idx = grid_width * idx_y + idx_x ; float sum = 0 ; int k ; for ( k = - filterR ; k <= filterR ; k ++ ) { int d = idx_x + k ; if ( d >= 0 && d < imageW ) { sum += input [ idx_y * imageW + d ] * filter [ filterR - k ] ; } } output [ idx ] = sum ; }
__global__ void castImageToGrayScale ( unsigned char * ucharImage , unsigned char * grayImage , int imageWidth , int imageHeight , int channels ) { int w = threadIdx . x + blockDim . x * blockIdx . x ; int h = threadIdx . y + blockDim . y * blockIdx . y ; int idx = imageWidth * h + w ; if ( w < imageWidth && h < imageHeight ) { unsigned char r = ucharImage [ idx * channels ] ; unsigned char g = ucharImage [ idx * channels + 1 ] ; unsigned char b = ucharImage [ idx * channels + 2 ] ; grayImage [ idx ] = ( unsigned char ) ( 0.21f * r + 0.71f * g + 0.07f * b ) ; } }
__global__ void GPU_array_rowKernel ( double * input , double * output , int length ) { int xCuda = blockDim . x * blockIdx . x + threadIdx . x ; int yCuda = blockDim . y * blockIdx . y + threadIdx . y ; int idx = yCuda * length + xCuda ; if ( xCuda >= length || yCuda >= length ) return ; if ( xCuda == 0 || xCuda == length - 1 ) { output [ idx ] = 0 ; return ; } output [ idx ] = input [ idx ] ; output [ idx ] += xCuda == 0 ? 0 : input [ idx - 1 ] ; output [ idx ] += xCuda == length - 1 ? 0 : input [ idx + 1 ] ; }
__global__ void waterElevationToDepth ( const int nx_ , const int ny_ , float * h_ptr_ , int h_pitch_ , float * Bm_ptr_ , int Bm_pitch_ ) { int ti = blockIdx . x * blockDim . x + threadIdx . x ; int tj = blockIdx . y * blockDim . y + threadIdx . y ; if ( ti < nx_ && tj < ny_ ) { float * const h_row = ( float * ) ( ( char * ) h_ptr_ + h_pitch_ * tj ) ; float * const Bm_row = ( float * ) ( ( char * ) Bm_ptr_ + Bm_pitch_ * tj ) ; h_row [ ti ] -= Bm_row [ ti ] ; } }
__global__ void kernel_softmax ( float * x , int r , int c ) { unsigned int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i >= r ) return ; float temp1 = 0. , temp2 = 0. ; for ( int j = 0 ; j < c ; j ++ ) temp1 = max ( x [ i * c + j ] , temp1 ) ; for ( int j = 0 ; j < c ; j ++ ) { x [ i * c + j ] = expf ( x [ i * c + j ] - temp1 ) ; temp2 += x [ i * c + j ] ; } for ( int j = 0 ; j < c ; j ++ ) x [ i * c + j ] /= temp2 ; }
__global__ void cuda_Adam_step_kernel ( float * grad , float * data , float * m , float * v , bool decay , float weight_decay , float beta1 , float beta2 , float eps , float step_size , int varsize ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= varsize ) return ; float g = grad [ i ] ; if ( decay ) g += weight_decay * data [ i ] ; m [ i ] = beta1 * m [ i ] + ( 1.0 - beta1 ) * g ; v [ i ] = beta2 * v [ i ] + ( 1.0 - beta2 ) * g * g ; data [ i ] -= step_size * m [ i ] / ( sqrtf ( v [ i ] ) + eps ) ; }
__global__ void flatten_kernel ( int N , float * x , int spatial , int layers , int batch , int forward , float * out ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i >= N ) return ; int in_s = i % spatial ; i = i / spatial ; int in_c = i % layers ; i = i / layers ; int b = i ; int i1 = b * layers * spatial + in_c * spatial + in_s ; int i2 = b * layers * spatial + in_s * layers + in_c ; if ( forward ) out [ i2 ] = x [ i1 ] ; else out [ i1 ] = x [ i2 ] ; }
__global__ void Kernel_Softmax ( float * dev_x , const int r , const int c ) { unsigned int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i >= r ) return ; float temp1 = 0. , temp2 = 0. ; for ( int j = 0 ; j < c ; j ++ ) temp1 = max ( dev_x [ i * c + j ] , temp1 ) ; for ( int j = 0 ; j < c ; j ++ ) { dev_x [ i * c + j ] = expf ( dev_x [ i * c + j ] - temp1 ) ; temp2 += dev_x [ i * c + j ] ; } for ( int j = 0 ; j < c ; j ++ ) dev_x [ i * c + j ] /= temp2 ; }
__global__ void deInterleave_kernel2 ( float * d_X_out , float * d_Y_out , char * d_XY_in , int pitch_out , int pitch_in , int width , int height ) { unsigned int x = blockIdx . x * blockDim . x + threadIdx . x ; unsigned int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( ( x < width ) & ( y < height ) ) { float * data = ( float * ) ( d_XY_in + y * pitch_in ) + 2 * x ; * ( ( float * ) ( ( char * ) d_X_out + y * pitch_out ) + x ) = data [ 0 ] ; * ( ( float * ) ( ( char * ) d_Y_out + y * pitch_out ) + x ) = data [ 1 ] ; } }
__global__ void cuda_laplace_filter ( float * Img , float * laplace , float _dz , float _dx , int npml , int nnz , int nnx ) { int i1 = threadIdx . x + blockDim . x * blockIdx . x ; int i2 = threadIdx . y + blockDim . y * blockIdx . y ; int id = i1 + i2 * nnz ; float diff1 = 0.0f ; float diff2 = 0.0f ; if ( i1 >= npml && i1 < nnz - npml && i2 >= npml && i2 < nnx - npml ) { diff1 = Img [ id + 1 ] - 2.0 * Img [ id ] + Img [ id - 1 ] ; diff2 = Img [ id + nnz ] - 2.0 * Img [ id ] + Img [ id - nnz ] ; } laplace [ id ] = _dz * _dz * diff1 + _dx * _dx * diff2 ; }
__global__ void distanceMatFinal ( long int totalPixels , int availablePixels , int outPixelOffset , float * distMat ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = blockDim . x * gridDim . x ; for ( long int i = index ; i < availablePixels ; i += stride ) { float sum = 0.0 ; float max = 0.0 ; for ( long int j = 0 ; j < totalPixels ; j ++ ) { float element = distMat [ i * totalPixels + j ] ; if ( element > max ) max = element ; sum += element ; } sum += max ; for ( long int j = 0 ; j < totalPixels ; j ++ ) { if ( ( i + outPixelOffset ) == j ) distMat [ i * totalPixels + j ] = max / sum ; else distMat [ i * totalPixels + j ] /= sum ; } } }
__global__ void permuteData2 ( const float * input , float * output , int num , int devideNum , int featureSize , int priorNum , int batchSize ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= num ) { return ; } int numPerbatch = num * devideNum * priorNum ; for ( int s = 0 ; s < batchSize ; s ++ ) { for ( int i = 0 ; i < priorNum ; i ++ ) { for ( int j = 0 ; j < devideNum ; j ++ ) { output [ s * numPerbatch + tid * priorNum * devideNum + i * devideNum + j ] = input [ s * numPerbatch + ( i * devideNum * featureSize ) + ( j * featureSize ) + tid ] ; } } } }
__global__ void permuteDataTorch ( const float * input , float * output , int num , int devideNum , int featureSize , int priorNum , int batchSize ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= num ) { return ; } int numPerbatch = num * devideNum * priorNum ; for ( int s = 0 ; s < batchSize ; s ++ ) { for ( int i = 0 ; i < priorNum ; i ++ ) { for ( int j = 0 ; j < devideNum ; j ++ ) { output [ s * numPerbatch + tid * priorNum * devideNum + i * devideNum + j ] = input [ s * numPerbatch + ( i * devideNum * featureSize ) + ( j * featureSize ) + tid ] ; } } } }
__global__ void conv1x1 ( int input_channels , int input_size , int n , float * input_im , float * filter_weight , float * filter_bias , float * output_im ) { int filter_index = blockIdx . x * blockDim . x + threadIdx . x ; if ( filter_index < n ) { filter_weight += filter_index * input_channels ; float bias = filter_bias [ filter_index ] ; output_im += filter_index * input_size * input_size ; for ( int i = 0 ; i < input_size ; i ++ ) { for ( int j = 0 ; j < input_size ; j ++ ) { float tmp = bias ; for ( int k = 0 ; k < input_channels ; k ++ ) { tmp += input_im [ k * input_size * input_size + i * input_size + j ] * filter_weight [ k ] ; } output_im [ i * input_size + j ] = ( tmp > 0.0 ) ? tmp : 0.0 ; } } } }
__global__ void Kernel_Softmax_seg ( float * dev_x , const int c , const int size ) { unsigned int i = blockDim . x * blockIdx . x + threadIdx . x ; int N = size ; float temp = 0. ; while ( i < N ) { for ( int j = 0 ; j < c ; j ++ ) temp = max ( dev_x [ j * size + i ] , temp ) ; for ( int j = 0 ; j < c ; j ++ ) dev_x [ j * size + i ] = expf ( dev_x [ j * size + i ] - temp ) ; temp = 0.0 ; for ( int j = 0 ; j < c ; j ++ ) temp += dev_x [ j * size + i ] ; for ( int j = 0 ; j < c ; j ++ ) dev_x [ j * size + i ] /= temp ; i += gridDim . x * blockDim . x ; } }
__global__ void pad_input ( float * f_in , float * f_out , int H , int W , int D , int pad ) { int col = blockIdx . x * blockDim . x + threadIdx . x ; int row = blockIdx . y * blockDim . y + threadIdx . y ; int dep = blockIdx . z * blockDim . z + threadIdx . z ; int new_H = H + 2 * pad ; int new_W = W + 2 * pad ; int i = dep * new_H * new_W + col * new_W + row ; int j = dep * H * W + ( col - pad ) * W + ( row - pad ) ; if ( col < new_H && row < new_W && dep < D ) { if ( ( col < pad || col > H + pad - 1 ) || ( row < pad || row > W + pad - 1 ) ) f_out [ i ] = 0 ; else f_out [ i ] = f_in [ j ] ; } }
__global__ void waterDepthToElevation ( const int nx_ , const int ny_ , float * w_ptr_ , int w_pitch_ , float * h_ptr_ , int h_pitch_ , float * Bm_ptr_ , int Bm_pitch_ ) { const int ti = blockIdx . x * blockDim . x + threadIdx . x ; const int tj = blockIdx . y * blockDim . y + threadIdx . y ; if ( ti < nx_ && tj < ny_ ) { float * const h_row = ( float * ) ( ( char * ) h_ptr_ + h_pitch_ * tj ) ; float * const Bm_row = ( float * ) ( ( char * ) Bm_ptr_ + Bm_pitch_ * tj ) ; float * const w_row = ( float * ) ( ( char * ) w_ptr_ + w_pitch_ * tj ) ; w_row [ ti ] = h_row [ ti ] + Bm_row [ ti ] ; } }
__global__ void invalidateFlow_kernel ( float * modFlowX , float * modFlowY , const float * constFlowX , const float * constFlowY , int width , int height , float cons_thres ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( x < width && y < height ) { int ind = y * width + x ; float mFX = modFlowX [ ind ] ; float mFY = modFlowY [ ind ] ; float cFX = constFlowX [ ind ] ; float cFY = constFlowY [ ind ] ; float err = ( mFX - cFX ) * ( mFX - cFX ) + ( mFY - cFY ) * ( mFY - cFY ) ; if ( err > cons_thres ) { mFX = 0 ; mFY = 0 ; } modFlowX [ ind ] = mFX ; modFlowY [ ind ] = mFY ; } }
__global__ void cudaRunComplexFilter ( float * I , float * Q , int samplesLength , float * hr , float * hi , int filterLength , float * filtered_I , float * filtered_Q , int convLength ) { int sampleIndex = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( sampleIndex >= convLength ) return ; int index ; float sumI , sumQ ; sumI = 0 ; sumQ = 0 ; for ( int j = sampleIndex - filterLength + 1 ; j <= sampleIndex ; j ++ ) { index = sampleIndex - j ; if ( ( j < samplesLength ) && ( j >= 0 ) ) { sumI += ( I [ j ] * hr [ index ] ) - ( Q [ j ] * hi [ index ] ) ; sumQ += ( I [ j ] * hi [ index ] ) + ( Q [ j ] * hr [ index ] ) ; } } filtered_I [ sampleIndex ] = sumI ; filtered_Q [ sampleIndex ] = sumQ ; }
__global__ void opL21 ( float * vec , float * vec1 , long depth , long rows , long cols ) { unsigned long x = threadIdx . x + blockIdx . x * blockDim . x ; unsigned long y = threadIdx . y + blockIdx . y * blockDim . y ; unsigned long z = threadIdx . z + blockIdx . z * blockDim . z ; unsigned long long i = z * rows * cols + y * cols + x ; unsigned long long j = z * rows * cols + x ; unsigned long size2d = cols ; unsigned long size3d = depth * rows * cols + rows * cols + cols ; if ( x >= cols || y >= rows || z >= depth ) return ; if ( i + cols + 1 >= size3d ) return ; vec [ i + cols ] = 0.25 * ( vec1 [ i + 1 ] + vec1 [ i ] + vec1 [ i + cols + 1 ] + vec1 [ i + cols ] ) ; if ( j + 1 >= size2d ) return ; vec [ j ] = ( vec1 [ j ] + vec1 [ j + 1 ] ) / 4 ; }
__global__ void reorg_kernel ( int N , float * x , int w , int h , int c , int batch , int stride , int forward , float * out ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i >= N ) return ; int in_index = i ; int in_w = i % w ; i = i / w ; int in_h = i % h ; i = i / h ; int in_c = i % c ; i = i / c ; int b = i % batch ; int out_c = c / ( stride * stride ) ; int c2 = in_c % out_c ; int offset = in_c / out_c ; int w2 = in_w * stride + offset % stride ; int h2 = in_h * stride + offset / stride ; int out_index = w2 + w * stride * ( h2 + h * stride * ( c2 + out_c * b ) ) ; if ( forward ) out [ out_index ] = x [ in_index ] ; else out [ in_index ] = x [ out_index ] ; }
__global__ void maxValExtract ( float * normM_c , float * normM1_c , long int image_size , float * d_projections , int * d_index , float a ) { __shared__ int pos [ 2048 ] ; __shared__ float val [ 2048 ] ; unsigned int tid = threadIdx . x ; unsigned int id = blockIdx . x * 2 * blockDim . x + threadIdx . x ; float faux , faux2 ; faux = ( ( a - normM_c [ id ] ) / a ) ; faux2 = ( ( a - normM_c [ id + blockDim . x ] ) / a ) ; if ( id < image_size && faux <= 1.0e-6 ) { val [ tid ] = normM1_c [ id ] ; pos [ tid ] = id ; } else { val [ tid ] = -1 ; } if ( id + blockDim . x < image_size && faux2 <= 1.0e-6 ) { val [ tid + blockDim . x ] = normM1_c [ id + blockDim . x ] ; pos [ tid + blockDim . x ] = id + blockDim . x ; } else { val [ tid + blockDim . x ] = -1 ; } __syncthreads ( ) ; for ( unsigned int s = blockDim . x ; s > 0 ; s >>= 1 ) { if ( tid < s ) { if ( val [ tid ] <= val [ tid + s ] ) { val [ tid ] = val [ tid + s ] ; pos [ tid ] = pos [ tid + s ] ; } } __syncthreads ( ) ; } d_projections [ blockIdx . x ] = val [ 0 ] ; d_index [ blockIdx . x ] = ( int ) pos [ 0 ] ; __syncthreads ( ) ; }
__global__ void returnResult ( const float * box , const float * score , const int * label , float * box_out , float * score_out , int * label_out , float score_thr , const int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( score [ tid ] < score_thr ) { score_out [ tid ] = 0 ; box_out [ tid * 4 + 0 ] = -1 ; box_out [ tid * 4 + 1 ] = -1 ; box_out [ tid * 4 + 2 ] = -1 ; box_out [ tid * 4 + 3 ] = -1 ; label_out [ tid ] = -1 ; } else { score_out [ tid ] = score [ tid ] ; box_out [ tid * 4 + 0 ] = box [ tid * 4 + 0 ] ; box_out [ tid * 4 + 1 ] = box [ tid * 4 + 1 ] ; box_out [ tid * 4 + 2 ] = box [ tid * 4 + 2 ] ; box_out [ tid * 4 + 3 ] = box [ tid * 4 + 3 ] ; label_out [ tid ] = label [ tid ] ; } }
__global__ void RyT ( float * R , float * T , float * P , float * Q ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; Q [ 0 + i * 3 ] = R [ 0 + 0 * 3 ] * P [ 0 + i * 3 ] + R [ 0 + 1 * 3 ] * P [ 1 + i * 3 ] + R [ 0 + 2 * 3 ] * P [ 2 + i * 3 ] + T [ 0 ] ; Q [ 1 + i * 3 ] = R [ 1 + 0 * 3 ] * P [ 0 + i * 3 ] + R [ 1 + 1 * 3 ] * P [ 1 + i * 3 ] + R [ 1 + 2 * 3 ] * P [ 2 + i * 3 ] + T [ 1 ] ; Q [ 2 + i * 3 ] = R [ 2 + 0 * 3 ] * P [ 0 + i * 3 ] + R [ 2 + 1 * 3 ] * P [ 1 + i * 3 ] + R [ 2 + 2 * 3 ] * P [ 2 + i * 3 ] + T [ 2 ] ; }
__global__ void primal_descent ( float * y1 , float * y2 , float * xbar , float sigma , int w , int h , int nc ) { int x = threadIdx . x + blockDim . x * blockIdx . x ; int y = threadIdx . y + blockDim . y * blockIdx . y ; if ( x < w && y < h ) { int i ; float x1 , x2 , val , norm ; for ( int z = 0 ; z < nc ; z ++ ) { i = x + w * y + w * h * z ; val = xbar [ i ] ; x1 = ( x + 1 < w ) ? ( xbar [ ( x + 1 ) + w * y + w * h * z ] - val ) : 0.f ; x2 = ( y + 1 < h ) ? ( xbar [ x + w * ( y + 1 ) + w * h * z ] - val ) : 0.f ; x1 = y1 [ i ] + sigma * x1 ; x2 = y2 [ i ] + sigma * x2 ; norm = sqrtf ( x1 * x1 + x2 * x2 ) ; y1 [ i ] = x1 / fmax ( 1.f , norm ) ; y2 [ i ] = x2 / fmax ( 1.f , norm ) ; } } }
__global__ void fractal ( const int width , const int frames , unsigned char * const pic ) { const long i = threadIdx . x + blockIdx . x * ( long ) blockDim . x ; if ( i > width * width * frames ) { return ; } const double Delta = 0.00304 ; const double xMid = -0.055846456 ; const double yMid = -0.668311119 ; const int frame = i / ( width * width ) ; double delta = Delta * pow ( 0.975 , frame ) ; const int col = i % width ; const double xMin = xMid - delta ; const double yMin = yMid - delta ; const double dw = 2.0 * delta / width ; const int row = ( i / width ) % width ; const double cy = yMin + row * dw ; const double cx = xMin + col * dw ; double x = cx ; double y = cy ; double x2 , y2 ; int count = 256 ; do { x2 = x * x ; y2 = y * y ; y = 2.0 * x * y + cy ; x = x2 - y2 + cx ; count -- ; } while ( ( count > 0 ) && ( ( x2 + y2 ) <= 5.0 ) ) ; pic [ frame * width * width + row * width + col ] = ( unsigned char ) count ; }
__global__ void Ring_kernel ( float * A , float * BP , int * corrAB , float * M , int ring , int c , int h , int w ) { int id1 = blockIdx . x * blockDim . x + threadIdx . x ; int size = h * w ; int ringSize = 2 * ring + 1 ; int ringPatch = ringSize * ringSize ; if ( id1 < size ) { int y1 = id1 / w , x1 = id1 % w ; int y2 = corrAB [ 2 * id1 + 1 ] , x2 = corrAB [ 2 * id1 + 0 ] ; for ( int dx = - ring ; dx <= ring ; dx ++ ) for ( int dy = - ring ; dy <= ring ; dy ++ ) { int pIdx = ( dy + ring ) * ringSize + ( dx + ring ) ; int _x2 = x2 + dx , _y2 = y2 + dy ; if ( _x2 >= 0 && _x2 < w && _y2 >= 0 && _y2 < h ) { for ( int dc = 0 ; dc < c ; dc ++ ) { M [ ( dc * size + y1 * w ) * ringPatch + pIdx * w + x1 ] = BP [ dc * size + _y2 * w + _x2 ] ; } } } } return ; }
__global__ void convolution_kernel_v1 ( float * device_outputMatrix , float * device_inputMatrix , float * device_filter , int imageRows , int imageColumns , int filterSize ) { int index_x = blockDim . x * blockIdx . x + threadIdx . x ; int index_y = blockDim . y * blockIdx . y + threadIdx . y ; float convolvedValue = 0.f ; for ( int eachFilterRow = - filterSize / 2 ; eachFilterRow <= filterSize / 2 ; ++ eachFilterRow ) { for ( int eachFilterColumn = - filterSize / 2 ; eachFilterColumn <= filterSize / 2 ; ++ eachFilterColumn ) { int imageRow = index_y + eachFilterRow ; int imageColumn = index_x + eachFilterColumn ; float pixelValue = ( imageRow >= 0 && imageRow < imageRows && imageColumn >= 0 && imageColumn < imageColumns ) ? device_inputMatrix [ imageRow * imageColumns + imageColumn ] : 0.f ; float filterValue = device_filter [ ( eachFilterRow + filterSize / 2 ) * filterSize + eachFilterColumn + filterSize / 2 ] ; convolvedValue += pixelValue * filterValue ; } } device_outputMatrix [ index_y * imageColumns + index_x ] = convolvedValue ; }
__global__ void opLadj1 ( float * vec , float * vec1 , float * vec2 , float * vec3 , long depth , long rows , long cols ) { unsigned long x = threadIdx . x + blockIdx . x * blockDim . x ; unsigned long y = threadIdx . y + blockIdx . y * blockDim . y ; unsigned long z = threadIdx . z + blockIdx . z * blockDim . z ; unsigned long long i = z * rows * cols + y * cols + x ; unsigned long long j = z * rows * cols + x ; unsigned long size2d = cols ; unsigned long size3d = depth * rows * cols + rows * cols + cols ; if ( x >= cols || y >= rows || z >= depth ) return ; if ( i + cols + 1 >= size3d ) return ; vec [ i + cols ] = vec1 [ i + cols ] + 0.25 * ( vec2 [ i + cols ] + vec2 [ i ] + vec2 [ i + cols + 1 ] + vec2 [ i + 1 ] ) + 0.5 * ( vec3 [ i + cols ] + vec3 [ i + cols + 1 ] ) ; if ( j + 1 >= size2d ) return ; vec [ j ] = vec1 [ j ] + ( vec2 [ j ] + vec2 [ j + 1 ] ) / 4 + ( vec3 [ j ] + vec3 [ j + 1 ] ) / 2 ; }
__global__ void dual_ascent ( float * xn , float * xbar , float * y1 , float * y2 , float * img , float tau , float lambda , float theta , int w , int h , int nc ) { int x = threadIdx . x + blockDim . x * blockIdx . x ; int y = threadIdx . y + blockDim . y * blockIdx . y ; if ( x < w && y < h ) { int i ; float d1 , d2 , val ; for ( int z = 0 ; z < nc ; z ++ ) { i = x + w * y + w * h * z ; d1 = ( x + 1 < w ? y1 [ i ] : 0.f ) - ( x > 0 ? y1 [ ( x - 1 ) + w * y + w * h * z ] : 0.f ) ; d2 = ( y + 1 < h ? y2 [ i ] : 0.f ) - ( y > 0 ? y2 [ x + w * ( y - 1 ) + w * h * z ] : 0.f ) ; val = xn [ i ] ; xn [ i ] = ( ( val + tau * ( d1 + d2 ) ) + tau * lambda * img [ i ] ) / ( 1.f + tau * lambda ) ; xbar [ i ] = xn [ i ] + theta * ( xn [ i ] - val ) ; } } }
__global__ void convoluteGPU ( float * dData , float * hData , int height , int width , float * mask , int masksize ) { int row = threadIdx . x + blockIdx . x * blockDim . x ; int col = threadIdx . y + blockIdx . y * blockDim . y ; int S = ( masksize - 1 ) / 2 ; float sum = 0 ; int pixPos = row * width + col ; dData [ pixPos ] = 0.0 ; if ( row < height && col < width ) { for ( int maskrow = - S ; maskrow <= S ; maskrow ++ ) { for ( int maskcol = - S ; maskcol <= S ; maskcol ++ ) { int pixP = ( row + maskrow ) * width + ( col + maskcol ) ; int maskP = ( maskrow + S ) * masksize + ( maskcol + S ) ; if ( pixP < height * width && pixP > 0 && maskP < masksize * masksize ) { sum += mask [ maskP ] * hData [ pixP ] ; } } } dData [ pixPos ] = sum ; if ( dData [ pixPos ] < 0 ) { dData [ pixPos ] = 0 ; } else if ( dData [ pixPos ] > 1 ) { dData [ pixPos ] = 1 ; } } }
__global__ void opLadj2 ( float * vec , float * vec1 , float * vec2 , float * vec3 , long depth , long rows , long cols ) { unsigned long x = threadIdx . x + blockIdx . x * blockDim . x ; unsigned long y = threadIdx . y + blockIdx . y * blockDim . y ; unsigned long z = threadIdx . z + blockIdx . z * blockDim . z ; unsigned long long i = z * rows * cols + y * cols + x ; unsigned long long j = z * rows * cols + y * cols ; unsigned long size2d = z * rows * cols + cols * rows ; unsigned long size3d = depth * rows * cols + rows * cols + cols ; if ( x >= cols || y >= rows || z >= depth ) return ; if ( i + cols + 1 >= size3d ) return ; vec [ i + 1 ] = vec1 [ i + 1 ] + 0.25 * ( vec2 [ i + 1 ] + vec2 [ i ] + vec2 [ i + cols + 1 ] + vec2 [ i + cols ] ) + 0.5 * ( vec3 [ i + 1 ] + vec3 [ i + cols + 1 ] ) ; if ( j + cols >= size2d ) return ; vec [ j ] = vec1 [ j ] + ( vec2 [ j ] + vec2 [ j + cols ] ) / 4 + ( vec3 [ j ] + vec3 [ j + cols ] ) / 2 ; }
__global__ void cuda_Cross<unk> py_forward_A_kernel ( float * logits_data , float * logits_grad , bool training , int num_classes , int * truth , int * count , float * thread_loss , int size ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= size ) return ; if ( truth [ i ] < 0 ) { count [ i ] = 0 ; return ; } float * logit = & logits_data [ i * num_classes ] ; float max_logit = -1e30 , sum_exp = 0 ; for ( int j = 0 ; j < num_classes ; j ++ ) max_logit = fmax ( max_logit , logit [ j ] ) ; for ( int j = 0 ; j < num_classes ; j ++ ) { logit [ j ] -= max_logit ; sum_exp += expf ( logit [ j ] ) ; } if ( training ) { for ( int j = 0 ; j < num_classes ; j ++ ) { float prob = expf ( logit [ j ] ) / sum_exp ; logits_grad [ i * num_classes + j ] = prob ; } logits_grad [ i * num_classes + truth [ i ] ] -= 1.0 ; } count [ i ] = 1 ; thread_loss [ i ] = logf ( sum_exp ) - logit [ truth [ i ] ] ; }
__global__ void cuInsertionSort ( float * dist , long * ind , int width , int height , int k ) { int l , i , j ; float * p_dist ; long * p_ind ; float curr_dist , max_dist ; long curr_row , max_row ; unsigned int xIndex = blockIdx . x * blockDim . x + threadIdx . x ; if ( xIndex < width ) { p_dist = dist + xIndex ; p_ind = ind + xIndex ; max_dist = p_dist [ 0 ] ; p_ind [ 0 ] = 1 ; for ( l = 1 ; l < k ; l ++ ) { curr_row = l * width ; curr_dist = p_dist [ curr_row ] ; if ( curr_dist < max_dist ) { i = l - 1 ; for ( int a = 0 ; a < l - 1 ; a ++ ) { if ( p_dist [ a * width ] > curr_dist ) { i = a ; break ; } } for ( j = l ; j > i ; j -- ) { p_dist [ j * width ] = p_dist [ ( j - 1 ) * width ] ; p_ind [ j * width ] = p_ind [ ( j - 1 ) * width ] ; } p_dist [ i * width ] = curr_dist ; p_ind [ i * width ] = l + 1 ; } else { p_ind [ l * width ] = l + 1 ; } max_dist = p_dist [ curr_row ] ; } max_row = ( k - 1 ) * width ; for ( l = k ; l < height ; l ++ ) { curr_dist = p_dist [ l * width ] ; if ( curr_dist < max_dist ) { i = k - 1 ; for ( int a = 0 ; a < k - 1 ; a ++ ) { if ( p_dist [ a * width ] > curr_dist ) { i = a ; break ; } } for ( j = k - 1 ; j > i ; j -- ) { p_dist [ j * width ] = p_dist [ ( j - 1 ) * width ] ; p_ind [ j * width ] = p_ind [ ( j - 1 ) * width ] ; } p_dist [ i * width ] = curr_dist ; p_ind [ i * width ] = l + 1 ; max_dist = p_dist [ max_row ] ; } } } }
__global__ void conv2 ( float * A , float * kernel , int inputSize , int depth , int kernelSize , int stride , int pad , float * B , int outputSize ) { int i = threadIdx . x + blockDim . x * blockIdx . x ; int j = threadIdx . y + blockDim . y * blockIdx . y ; if ( ! ( i < outputSize ) || ! ( j < outputSize ) ) return ; int Ai = i * stride ; int Aj = j * stride ; int startk = ( pad - Ai ) < 0 ? 0 : pad - Ai ; int endk = kernelSize < ( inputSize + pad - Ai ) ? kernelSize : ( inputSize + pad - Ai ) ; int startl = ( pad - Aj ) < 0 ? 0 : pad - Aj ; int endl = kernelSize < ( inputSize + pad - Aj ) ? kernelSize : ( inputSize + pad - Aj ) ; float sum = 0 ; for ( int d = 0 ; d < depth ; d ++ ) { for ( int k = startk ; k < endk ; k ++ ) { for ( int l = startl ; l < endl ; l ++ ) { sum += A [ d * inputSize * inputSize + ( Ai + k - pad ) * inputSize + Aj + l - pad ] * kernel [ d * kernelSize * kernelSize + k * kernelSize + l ] ; } } B [ d * outputSize * outputSize + i * outputSize + j ] = sum ; } B [ i * outputSize + j ] = sum ; }
__global__ void get_positive_data ( const float * all_box , const float * all_scores , const float * all_conf , const int * conf_inds , float * positive_box , float * positive_scores , float * positive_conf , int dims , int clsNum ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( conf_inds [ tid ] != ( -1 ) ) { positive_box [ tid * 4 + 0 ] = all_box [ tid * 4 + 0 ] ; positive_box [ tid * 4 + 1 ] = all_box [ tid * 4 + 1 ] ; positive_box [ tid * 4 + 2 ] = all_box [ tid * 4 + 2 ] ; positive_box [ tid * 4 + 3 ] = all_box [ tid * 4 + 3 ] ; for ( int i = 0 ; i < clsNum ; i ++ ) { positive_scores [ tid * clsNum + i ] = all_scores [ tid * clsNum + i ] ; } positive_conf [ tid ] = all_conf [ tid ] ; } else { positive_box [ tid * 4 + 0 ] = 0 ; positive_box [ tid * 4 + 1 ] = 0 ; positive_box [ tid * 4 + 2 ] = 0 ; positive_box [ tid * 4 + 3 ] = 0 ; for ( int i = 0 ; i < clsNum ; i ++ ) { positive_scores [ tid * clsNum + i ] = ( -1 ) ; } positive_conf [ tid ] = ( -1 ) ; } }
__global__ void cudaChoiLee ( float * xi , float * xq , float * sr , float * si , int N , float * L ) { int u = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( u >= N ) return ; float uSum = 0 ; float r_i , r_q , rconj_i , rconj_q ; float s_i , s_q , sconj_i , sconj_q ; float rsum_i , rsum_q , ssum_i , ssum_q ; float ksum_i , ksum_q ; for ( int i = 0 ; i < N ; i ++ ) { ksum_i = 0 ; ksum_q = 0 ; for ( int k = 0 ; k < N - i ; k ++ ) { r_i = xi [ u + k + i ] ; r_q = xq [ u + k + i ] ; rconj_i = xi [ u + k ] ; rconj_q = xq [ u + k ] * ( -1 ) ; s_i = sr [ k ] ; s_q = si [ k ] ; sconj_i = sr [ k + i ] ; sconj_q = si [ k + i ] * ( -1 ) ; rsum_i = ( r_i * rconj_i ) - ( r_q * rconj_q ) ; rsum_q = ( r_i * rconj_q ) + ( r_q * rconj_i ) ; ssum_i = ( s_i * sconj_i ) - ( s_q * sconj_q ) ; ssum_q = ( s_i * sconj_q ) + ( s_q * sconj_i ) ; ksum_i += ( rsum_i * ssum_i ) - ( rsum_q * ssum_q ) ; ksum_q += ( rsum_i * ssum_q ) + ( rsum_q * ssum_i ) ; } uSum += sqrt ( ( ksum_i * ksum_i ) + ( ksum_q * ksum_q ) ) ; } L [ u ] = uSum ; }
__global__ void calculateOuterSumsNew ( float * innerSums , float * L , int uLength ) { int u = blockDim . x * blockIdx . x + threadIdx . x ; if ( u >= uLength ) return ; float real , imag , u_sum ; int realIdx = 2 * u ; int imagIdx = realIdx + 1 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum = ( real * real ) + ( imag * imag ) ; realIdx += 64 ; imagIdx += 64 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum += ( real * real ) + ( imag * imag ) ; realIdx += 64 ; imagIdx += 64 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum += ( real * real ) + ( imag * imag ) ; realIdx += 64 ; imagIdx += 64 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum += ( real * real ) + ( imag * imag ) ; realIdx += 64 ; imagIdx += 64 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum += ( real * real ) + ( imag * imag ) ; realIdx += 64 ; imagIdx += 64 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum += ( real * real ) + ( imag * imag ) ; realIdx += 64 ; imagIdx += 64 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum += ( real * real ) + ( imag * imag ) ; realIdx += 64 ; imagIdx += 64 ; real = innerSums [ realIdx ] ; imag = innerSums [ imagIdx ] ; u_sum += ( real * real ) + ( imag * imag ) ; L [ u ] = u_sum ; }
__global__ void nlf_right_forward ( const int n , const float * filters , const int channel , const int height , const int width , const int wsize , float * top_data ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index * step ; int fbase = index / channel * wsize * step ; for ( int col = 0 ; col < width ; col ++ ) { for ( int row = 0 ; row < height ; row ++ ) { float temp = 0 ; int r = row ; int c = col ; int shift = 0 * step + row * width + col ; temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; r = row ; c = col - 1 ; shift = 1 * step + row * width + col ; if ( c >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row - 1 ; c = col - 1 ; shift = 2 * step + row * width + col ; if ( c >= 0 && r >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row + 1 ; c = col - 1 ; shift = 3 * step + row * width + col ; if ( c >= 0 && r < height ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row - 1 ; c = col ; shift = 4 * step + row * width + col ; if ( r >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; top_data [ base + row * width + col ] = temp ; } } }
__global__ void nlf_filter_right_backward ( const int n , const float * bottom_data , const float * top_data , const float * temp_diff , const int channel , const int height , const int width , const int wsize , float * filters_diff ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index / step * step * channel + index % step ; int fbase = index / step * step * wsize + index % step ; int row = index % step / width ; int col = index % step % width ; for ( int i = 0 ; i < channel ; i ++ ) { filters_diff [ fbase ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col - 1 >= 0 ) filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * top_data [ base - 1 + i * step ] ; else filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col - 1 >= 0 && row - 1 >= 0 ) filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * top_data [ base - width - 1 + i * step ] ; else filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col - 1 >= 0 && row + 1 < height ) filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * top_data [ base + width - 1 + i * step ] ; else filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row - 1 >= 0 ) filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * top_data [ base - width + i * step ] ; else filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; } }
__global__ void nlf_filter_up_backward ( const int n , const float * bottom_data , const float * top_data , const float * temp_diff , const int channel , const int height , const int width , const int wsize , float * filters_diff ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index / step * step * channel + index % step ; int fbase = index / step * step * wsize + index % step ; int row = index % step / width ; int col = index % step % width ; for ( int i = 0 ; i < channel ; i ++ ) { filters_diff [ fbase ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row + 1 < height ) filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * top_data [ base + width + i * step ] ; else filters_diff [ fbase + step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row + 1 < height && col - 1 >= 0 ) filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * top_data [ base + width - 1 + i * step ] ; else filters_diff [ fbase + 2 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( row + 1 < height && col + 1 < width ) filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * top_data [ base + width + 1 + i * step ] ; else filters_diff [ fbase + 3 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; if ( col + 1 < width ) filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * top_data [ base + 1 + i * step ] ; else filters_diff [ fbase + 4 * step ] += temp_diff [ base + i * step ] * bottom_data [ base + i * step ] ; } }
__global__ void nlf_left_forward ( const int n , const float * filters , const int channel , const int height , const int width , const int wsize , float * top_data ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index >= n ) { return ; } int step = height * width ; int base = index * step ; int fbase = index / channel * wsize * step ; for ( int col = width - 1 ; col >= 0 ; col -- ) { for ( int row = height - 1 ; row >= 0 ; row -- ) { float temp = 0 ; int r = row ; int c = col ; int shift = 0 * step + row * width + col ; temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; r = row ; c = col + 1 ; shift = 1 * step + row * width + col ; if ( c < width ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row - 1 ; c = col + 1 ; shift = 2 * step + row * width + col ; if ( c < width && r >= 0 ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row + 1 ; c = col + 1 ; shift = 3 * step + row * width + col ; if ( c < width && r < height ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; r = row + 1 ; c = col ; shift = 4 * step + row * width + col ; if ( r < height ) temp += top_data [ base + r * width + c ] * filters [ fbase + shift ] ; else temp += top_data [ base + row * width + col ] * filters [ fbase + shift ] ; top_data [ base + row * width + col ] = temp ; } } }
